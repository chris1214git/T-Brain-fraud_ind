{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [玉山人工智慧公開挑戰賽2019秋季賽,真相只有一個 -『信用卡盜刷偵測』](https://tbrain.trendmicro.com.tw/Competitions/Details/10)\n",
    "\n",
    "## <font color=red>任務:預測某刷卡交易是否為盜刷</font>\n",
    "\n",
    "### Task Schedule:\n",
    "1. 讀取資料,將字串轉換成int\n",
    "2. EDA(exploratory data analysis)\n",
    "3. Feature engineering\n",
    "4. 訓練模型,調整參數(預計使用lgb，速度較快)\n",
    "5. 嘗試使用不同模型,做Ensamble(blending, stacking)\n",
    "6. Anomaly detection\n",
    "\n",
    "### 注意事項:\n",
    "1. 因為test data和train data時間不相關,在驗證時採取前60天訓練61~90天驗證,但仍需小心時間差異造成的影響\n",
    "\n",
    "### TODO:\n",
    "1. **EDA(見下方詳細解釋）,找出不適合作為training feature的特徵,加以轉化成高級特徵或刪除**\n",
    "2. **找data leakage**\n",
    "\n",
    "3. Anomaly detection: 看這類的模型能不能取代lgb(似乎是不行，盜刷數據並沒有那麼Anomaly）,但可以嘗試將Anomaly結果當成新feature\n",
    "\n",
    "### <font color=green>Results:</font>\n",
    "* 不做處理,直接丟lgb訓練 leaderboard score:0.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取,轉換字串成可以訓練的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = '../data'\n",
    "\n",
    "random_seed = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data.csv (1943452, 23)\n",
      "Null columns:\n",
      " fraud_ind    421665\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_list=['raw_data.csv','FE_data1.csv','FE_data2.csv','FE_data4.csv']\n",
    "\n",
    "data=[]\n",
    "for d in data_list:\n",
    "    x = pd.read_csv('../data/preprocess/{}'.format(d),index_col=False)\n",
    "    print(d,x.shape)\n",
    "    x_null = x.isnull().sum()\n",
    "    print(\"Null columns:\\n\",x_null[x_null>0])\n",
    "    \n",
    "    if (d=='FE_data1.csv') or (d=='FE_data2.csv'):\n",
    "        x=x.fillna(value=-1)\n",
    "        \n",
    "    data.append(x)\n",
    "\n",
    "all_data = pd.concat(data,axis=1)\n",
    "\n",
    "all_data_numsum = all_data.isnull().sum()\n",
    "print('ALL data null:')\n",
    "print(all_data_numsum[all_data_numsum>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA(觀察資料分佈, 找特徵工程的Idea）\n",
    "\n",
    "#### 方法（四種）:\n",
    "1. 比較train, validation data, test data各個feature(不包括fraud_ind)的分佈，看各個feature是否會隨時間不同而改變\n",
    "    * 見 function compare_distribution\n",
    "    * 檢查差異,目測結果: txkey有一點差異，其他幾乎一樣\n",
    "    \n",
    "\n",
    "2. 比較train, validation data各個feature相對於fraud_ind的關係，看他們與fraud_ind的關係是否會隨時間改變，如果會就不適合做training feature\n",
    "    * 見 function analze_distribution\n",
    "    * 檢查差異，目測結果:\n",
    "        * 明顯差異,應該去除,或應該抽取更高層的穩定特徵(TODO): csmcu,mcc,loctm_hr2\n",
    "        * 無法判讀:應該找出來並想辦法去除或找更高層特徵(TODO): 蠻多特徵都看不出來有沒有差,因為類別太多＝＝\n",
    "\n",
    "\n",
    "3. 比較normal, fraud data的各個feature分佈差異，找有問題的feature!\n",
    "    * 結果:\n",
    "\n",
    "\n",
    "4. 但這樣無法看出單一用戶在normal和fraud的關係，所以要另外印出檢查，看有fraud data的用戶，該資料特點在哪\n",
    "    * 根據時間順序,以每個bacno來看fraud情況(最重要,適合找data leakage!!!!)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀測 train, validation, test的distribution\n",
    "* 跟fraud_ind相關的，檢查train和validation就好（假設test data跟validation類似）\n",
    "* 每筆feature的distribution，檢查train,validation和test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analze_distribution(data, target_col, feature, data_test):\n",
    "#     if data[feature].nunique()!=data_test[feature].nunique():\n",
    "#         print('data nunique not the same')\n",
    "    print(feature)\n",
    "    plt.clf()\n",
    "    mean_data = data.groupby(feature)[target_col].mean()\n",
    "    mean_data_test = data_test.groupby(feature)[target_col].mean()\n",
    "    distribution_data = data[feature].value_counts(dropna=False)\n",
    "    distribution_data_test = data_test[feature].value_counts(dropna=False)\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2,figsize=(10,5))\n",
    "    axs[0,0].plot(mean_data.index, mean_data.values, marker='o')\n",
    "    axs[0,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[0,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[0,0].set_xlabel(feature)\n",
    "    \n",
    "    \n",
    "    bins = data[feature].nunique() if data[feature].nunique()<100 else 100\n",
    "    data[feature].hist(bins=bins,ax=axs[0,1])\n",
    "\n",
    "#     axs[0,1].bar(distribution_data.index, distribution_data.values, alpha=0.5)\n",
    "    axs[0,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[0,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[0,1].set_xlabel(feature)\n",
    "    \n",
    "    # Add text in figure coordinates\n",
    "    plt.figtext(0.5, 1,   'Train data plot', ha='center', va='center', fontsize=15)\n",
    "    plt.figtext(0.5, 0.5, 'Test data plot', ha='center', va='center', fontsize=15)\n",
    "    axs[1,0].plot(mean_data_test.index, mean_data_test.values, marker='o')\n",
    "    axs[1,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[1,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[1,0].set_xlabel(feature)\n",
    "    \n",
    "    bins = data_test[feature].nunique() if data_test[feature].nunique()<100 else 100\n",
    "    data_test[feature].hist(bins=bins,ax=axs[1,1])\n",
    "#     axs[1,1].bar(distribution_data_test.index, distribution_data_test.values, alpha=0.5)\n",
    "    distribution_data_test.hist\n",
    "    axs[1,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[1,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[1,1].set_xlabel(feature)\n",
    "    plt.tight_layout(pad=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = all_data[all_data['locdt']<=60]\n",
    "valid_d = all_data[(all_data['locdt']>60)&(all_data['locdt']<=90)]\n",
    "# 自己做的圖表function，畵資料分佈的bar chart和對fraud_ind的關係\n",
    "\n",
    "for c in train_d.columns:\n",
    "    analze_distribution(train_d,'fraud_ind',c,valid_d)\n",
    "\n",
    "\n",
    "from featexp import get_univariate_plots\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['hcefg'], bins=100, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['loctm_hr'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['flg_3dsmk'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['csmcu'], bins=100, data_test=valid_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查train和test data的分佈有哪裡不一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distribution(train_data,test_data,target_name):\n",
    "    target = train_data[target_name]\n",
    "    target2 = test_data[target_name]\n",
    "    print(target_name)\n",
    "    print('nunique train',target.nunique())\n",
    "    print('nunique test',target2.nunique())\n",
    "    print('max train',target.max())\n",
    "    print('max test',target2.max())\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "    \n",
    "    bins = target.nunique() if target.nunique()<100 else 100\n",
    "    target.hist(bins=bins,ax=axs[0])\n",
    "    \n",
    "    bins = target2.nunique() if target2.nunique()<100 else 100\n",
    "    target2.hist(bins=bins,ax=axs[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# for c in all_data.drop(columns='fraud_ind'):\n",
    "#     compare_distribution(all_data[all_data['locdt']<=90].drop(columns='fraud_ind'),all_data[all_data['locdt']>90].drop(columns='fraud_ind'),c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud資料和normal資料的各個特徵差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = all_data[all_data['fraud_ind']==0]\n",
    "fraud_data = all_data[all_data['fraud_ind']==1]\n",
    "\n",
    "# for c in normal_data.columns:\n",
    "#     print(c)\n",
    "#     plt.clf()\n",
    "#     fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "#     bins = normal_data[c].nunique() if normal_data[c].nunique()<100 else 100\n",
    "#     normal_data[c].hist(bins=bins,ax=axs[0])\n",
    "#     axs[0].set_title('Normal data')\n",
    "#     bins = fraud_data[c].nunique() if fraud_data[c].nunique()<100 else 100\n",
    "#     fraud_data[c].hist(bins=bins,ax=axs[1])\n",
    "#     axs[1].set_title('Fraud data')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀察large category的feature是否有很多種bacno的fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mchno_bacno_count = all_data.groupby('mchno').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())\n",
    "acqic_bacno_count = all_data.groupby('acqic').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())\n",
    "mcc_bacno_count = all_data.groupby('mcc').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mchno_bacno_count.sum())\n",
    "print(mchno_bacno_count[mchno_bacno_count>1].sort_values())\n",
    "\n",
    "print(acqic_bacno_count.sum())\n",
    "print(acqic_bacno_count[acqic_bacno_count>1].sort_values())\n",
    "\n",
    "print(mcc_bacno_count.sum())\n",
    "print(mcc_bacno_count[mcc_bacno_count>1].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以每個bacno來看fraud情況\n",
    "* 找出使用者被盜刷卡時，和一般的交易差在哪裡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacno_fraud_count = all_data.groupby('bacno').apply(lambda s:s[s['fraud_ind']==1]['bacno'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All fraud instance',all_data.fraud_ind.sum(skipna=True))\n",
    "print('{} different bacno'.format(bacno_fraud_count[bacno_fraud_count>0].shape[0]))\n",
    "\n",
    "bacno_fraud_count[(bacno_fraud_count<10)&(bacno_fraud_count>0)].hist(bins=9)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "bacno_fraud_count[(bacno_fraud_count>0)].hist(bins=100)\n",
    "\n",
    "## 幾乎都只被騙過一兩次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['locdt','cano','etymd','mchno','acqic','mcc','stocn','conam','fraud_ind']\n",
    "list2=['locdt','conam','cano','ecfg','stscd','stocn','mchno','fraud_ind']\n",
    "list3=['locdt','conam','cano','stocn','scity','flbmk','csmcu','ecfg','fraud_ind']\n",
    "list4=['locdt','cano','mcc','etymd','stocn','scity','csmcu','fraud_ind']\n",
    "# \n",
    "# ,'ecfg','etymd'\n",
    "bacno_10fraud_more_list = bacno_fraud_count[(bacno_fraud_count==1)].index\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# print(x)\n",
    "# pd.reset_option('display.max_rows')\n",
    "\n",
    "cnt=0\n",
    "for b in bacno_10fraud_more_list:\n",
    "    cnt+=1\n",
    "    if cnt>300:\n",
    "        break\n",
    "    print()\n",
    "    \n",
    "    print(b,all_data[all_data['bacno']==b]['fraud_ind'].sum())\n",
    "    if all_data[all_data['bacno']==b]['ecfg'].sum()>=0:\n",
    "        print(all_data[all_data['bacno']==b][list2].sort_values(by=['locdt']))\n",
    "        \n",
    "#     print(all_data[all_data['bacno']==b][list4].sort_values(by=['locdt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['locdt','cano','etymd','mchno','acqic','mcc','stocn','conam','fraud_ind']\n",
    "list2=['locdt','conam','cano','ecfg','stscd','stocn','mchno','fraud_ind']\n",
    "# \n",
    "# ,'ecfg'\n",
    "bacno_10fraud_more_list = bacno_fraud_count[(bacno_fraud_count>=10)].index\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# print(x)\n",
    "# pd.reset_option('display.max_rows')\n",
    "\n",
    "cnt=0\n",
    "for b in bacno_10fraud_more_list:\n",
    "    cnt+=1\n",
    "    if cnt>100:\n",
    "        break\n",
    "    print()\n",
    "    \n",
    "    print(b,all_data[all_data['bacno']==b]['fraud_ind'].sum())\n",
    "    print(all_data[all_data['bacno']==b][list2].sort_values(by=['locdt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "曾經在該國被盜刷的紀錄\n",
    "曾經在該mchno被盜刷的紀錄\n",
    "曾經在該mchno被盜刷\n",
    "最後一筆是不是盜刷\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查在test data中,新出現的類別有多少\n",
    "bacno, mchno, acqic, mcc, stocn, scity, csmcu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_category(x,target_name):\n",
    "    x_train = x[x['locdt']<=90][target_name].unique()\n",
    "    x_test = x[x['locdt']>90][target_name].unique()\n",
    "    \n",
    "    print(target_name)\n",
    "    print('{} categories in Training data:'.format(x_train.shape[0]))\n",
    "    print('{} categories in Testing data:'.format(x_test.shape[0]))\n",
    "\n",
    "    x_new_test=[]\n",
    "    for b in x_test:\n",
    "        if b not in x_train:\n",
    "            x_new_test.append(b)\n",
    "\n",
    "    print('{} new categories'.format(len(x_new_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 幾乎所有在test data的都是新用戶 乾\n",
    "## 所以重點特徵工程不能使用用戶過去的fraud記錄\n",
    "find_new_category(all_data,'bacno')\n",
    "\n",
    "## 幾乎都太多只出現在test data上的新類別,這樣得轉換可以用來訓練(TODO)\n",
    "find_new_category(all_data,'mchno')\n",
    "find_new_category(all_data,'acqic')\n",
    "find_new_category(all_data,'mcc')\n",
    "find_new_category(all_data,'stocn')\n",
    "find_new_category(all_data,'scity')\n",
    "find_new_category(all_data,'csmcu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([1,2,2,3,3])\n",
    "pd.Series.mode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../data/raw_data.csv',index_col=False)\n",
    "FE_data1 = pd.read_csv('../data/FE_data1.csv',index_col=False)\n",
    "FE_data3 = pd.read_csv('../data/FE_data3.csv',index_col=False)\n",
    "\n",
    "FE_data1 = FE_data1.fillna(value=-1)\n",
    "# FE_data3 = FE_data1.fillna(value=-1)\n",
    "\n",
    "all_data = pd.concat([raw_data,FE_data1,FE_data3],axis=1)\n",
    "print(raw_data.shape)\n",
    "print(FE_data1.shape)\n",
    "print(FE_data3.shape)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list1=['locdt','cano','etymd','mchno','acqic','mcc','stocn','conam','fraud_ind']\n",
    "\n",
    "list2=['locdt','conam','cano','ecfg','stscd','stocn','mchno','fraud_ind']\n",
    "\n",
    "bacno_10fraud_more_list = bacno_fraud_count[(bacno_fraud_count<=4)].index\n",
    "\n",
    "cnt=0\n",
    "for b in bacno_10fraud_more_list:\n",
    "    cnt+=1\n",
    "    if cnt>100:\n",
    "        break\n",
    "    print()\n",
    "    \n",
    "    print(b,all_data[all_data['bacno']==b]['fraud_ind'].sum())\n",
    "    print(all_data[all_data['bacno']==b][list2].sort_values(by=['locdt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame([[1,2,3],[2,3,4]])\n",
    "def hi(s):\n",
    "    return 1,2\n",
    "x = a.groupby([0]).apply(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x)\n",
    "# print(a)\n",
    "# b=(1,2)\n",
    "print(list([*b]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
