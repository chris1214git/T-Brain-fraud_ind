{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [玉山人工智慧公開挑戰賽2019秋季賽,真相只有一個 -『信用卡盜刷偵測』](https://tbrain.trendmicro.com.tw/Competitions/Details/10)\n",
    "\n",
    "## <font color=red>任務:預測某刷卡交易是否為盜刷</font>\n",
    "\n",
    "### Task Schedule:\n",
    "1. 讀取資料,將字串轉換成int\n",
    "2. EDA(exploratory data analysis)\n",
    "3. Feature engineering\n",
    "4. 訓練模型,調整參數(預計使用lgb，速度較快)\n",
    "5. 嘗試使用不同模型,做Ensamble(blending, stacking)\n",
    "6. Anomaly detection\n",
    "\n",
    "### 注意事項:\n",
    "1. 因為test data和train data時間不相關,在驗證時採取前60天訓練61~90天驗證,但仍需小心時間差異造成的影響\n",
    "\n",
    "### TODO:\n",
    "1. **EDA(見下方詳細解釋）,找出不適合作為training feature的特徵,加以轉化成高級特徵或刪除**\n",
    "2. **找data leakage**\n",
    "\n",
    "3. Anomaly detection: 看這類的模型能不能取代lgb(似乎是不行，盜刷數據並沒有那麼Anomaly）,但可以嘗試將Anomaly結果當成新feature\n",
    "\n",
    "### <font color=green>Results:</font>\n",
    "* 不做處理,直接丟lgb訓練 leaderboard score:0.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取,轉換字串成可以訓練的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = '../data'\n",
    "\n",
    "random_seed = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../data/all_data.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on LGB(未調參數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_list = ['bacno','locdt','loctm','cano','fraud_ind','iterm']\n",
    "#txkey大小, cano可能會重複所以重要？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_data[all_data['locdt']<=60].drop(columns=delete_list)\n",
    "y_train = all_data[all_data['locdt']<=60]['fraud_ind']\n",
    "X_test = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)].drop(columns=delete_list)\n",
    "y_test = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)]['fraud_ind']\n",
    "\n",
    "print(delete_list)\n",
    "print(X_train.shape)\n",
    "print(y_train.sum()/y_train.shape[0])\n",
    "print(y_test.sum()/y_test.shape[0])\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred = np.round(y_pred) # scikits f1 doesn't like probabilities\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print()\n",
    "    print('tn, fp, fn, tp')\n",
    "    print(tn, fp, fn, tp)\n",
    "    return 'f1', f1_score(y_true, y_pred), True\n",
    "\n",
    "param_dist_lgb = {\n",
    "#                   'num_leaves':45, \n",
    "#                   'max_depth':5, \n",
    "                  'learning_rate':0.1, \n",
    "                  'n_estimators':600,\n",
    "                  'objective': 'binary',\n",
    "#                   'subsample': 1, \n",
    "#                   'colsample_bytree': 0.5, \n",
    "#                   'lambda_l1': 0.1,\n",
    "#                   'lambda_l2': 0,\n",
    "#                   'min_child_weight': 1,\n",
    "                  'random_state': random_seed,\n",
    "                 }\n",
    "evals_result = {}\n",
    "\n",
    "lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "lgb_clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=True,\n",
    "        callbacks=[lgb.record_evaluation(evals_result)]\n",
    "        )\n",
    "y_test_pred = lgb_clf.predict(X_test)\n",
    "print('F1',f1_score(y_test, y_test_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='f1')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(lgb_clf, max_num_features=30)\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree...')  # one tree use categorical feature to split\n",
    "ax = lgb.plot_tree(lgb_clf, tree_index=3, figsize=(15, 15), show_info=['split_gain'])\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree with graphviz...')\n",
    "graph = lgb.create_tree_digraph(lgb_clf, tree_index=3, name='Tree4')\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on catboost\n",
    "* https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
    "* 研究有哪些可以用的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "test_data = catboost_pool = Pool(train_data, \n",
    "                                 train_labels)\n",
    "\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                           depth=2,\n",
    "                           learning_rate=1,\n",
    "                           loss_function='Logloss',\n",
    "                           verbose=True)\n",
    "# train the model\n",
    "model.fit(train_data, train_labels)\n",
    "# make the prediction using the resulting model\n",
    "preds_class = model.predict(test_data)\n",
    "preds_proba = model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用hinge loss(當SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['cents']\n",
    "# encoding data\n",
    "\n",
    "# GroupKfold\n",
    "# vanilla KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "# lgb_clf.fit(train_data,label_data)\n",
    "\n",
    "# result = lgb_clf.predict(test_data)\n",
    "# print(result.sum())\n",
    "# print(result.sum()/result.shape[0])\n",
    "# print(label_data.sum()/label_data.shape[0])\n",
    "\n",
    "# test_data_txkey = test_data['txkey'].copy()\n",
    "\n",
    "# import csv\n",
    "# with open('../prediction/submit_lgb.csv','w') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['txkey','fraud_ind'])\n",
    "#     for i in range(result.shape[0]):\n",
    "#         writer.writerow([test_data_txkey[i], result[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "* one class svm\n",
    "* isolation tree\n",
    "* replicator NN\n",
    "* Kmeans?\n",
    "* KNN(take too much time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作特徵\n",
    "XGB, PCA, Isolation Forest, Kmean距離？, oneclass SVM?\n",
    "當作新feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "param_dist_xgb = {'learning_rate':0.01, #默认0.3\n",
    "              'n_estimators':1000, #树的个数\n",
    "#               'max_depth':5,\n",
    "#               'min_child_weight':1,\n",
    "#               'gamma':0.2,\n",
    "#               'subsample':0.8,\n",
    "#               'colsample_bytree':0.8,\n",
    "#               'objective': 'binary:logistic', #逻辑回归损失函数\n",
    "#               'nthread':4,  #cpu线程数\n",
    "#               'scale_pos_weight':1,\n",
    "              'seed':random_seed}  #随机种子\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**param_dist_xgb)\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        early_stopping_rounds=600,\n",
    "        verbose=True,\n",
    "#         callbacks=[xgb.record_evaluation(evals_result)]\n",
    "        )\n",
    "\n",
    "print('F1',f1_score(y_test, xgb_clf.predict(X_test)))\n",
    "xgb_X_train = xgb_clf.apply(X_train)\n",
    "xgb_X_test = xgb_clf.apply(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualization in one person who has fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def PCA_plot(x,label):\n",
    "    x = x.drop(columns=)\n",
    "    \n",
    "    ## 應該先轉dummy,標準化,再PCA\n",
    "    dummy_list=['contp','etymd','stscd','hcefg']\n",
    "    dummy_list2=['stocn','scity','csmcu']#'mchno','acqic','mcc',\n",
    "    x[dummy_list] = x[dummy_list].astype(object)\n",
    "    x[dummy_list2] = x[dummy_list2].astype(object)\n",
    "    x = pd.get_dummies(x).drop(columns=['mchno','acqic'])    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    stdsc = StandardScaler() \n",
    "    x = stdsc.fit_transform(x)\n",
    "    print(x.shape,label.sum())\n",
    "\n",
    "    PCA_model = PCA(n_components=2)\n",
    "    train_data_pca = PCA_model.fit_transform(x)\n",
    "    train_data_pca1 = train_data_pca[label==1]\n",
    "    train_data_pca0 = train_data_pca[label==0]\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(train_data_pca1[:, 0], train_data_pca1[:, 1], c='r',label='fraud transaction',s=100)\n",
    "    plt.scatter(train_data_pca0[:, 0], train_data_pca0[:, 1], c='b',label='normal transaction',s=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "bacno_hasfraud = all_data[all_data['fraud_ind']==1]['bacno'].unique()\n",
    "print(bacno_hasfraud.shape[0])\n",
    "print(all_data[all_data['fraud_ind']==1].shape[0])\n",
    "\n",
    "for i in range(bacno_hasfraud.shape[0]):\n",
    "    if all_data[all_data['bacno']==bacno_hasfraud[i]].shape[0]>300:\n",
    "        print('Ploting PCA on bacno-{}'.format(bacno_hasfraud[i]))\n",
    "        PCA_plot(all_data[all_data['bacno']==bacno_hasfraud[i]],all_data[all_data['bacno']==bacno_hasfraud[i]]['fraud_ind'])\n",
    "\n",
    "## TSNE, Kmeans作圖?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "c_ratio = y_train.sum()/y_train.shape[0]\n",
    "# fit the model\n",
    "clf = IsolationForest(behaviour='new', max_samples=0.8, max_features=1,\n",
    "                      random_state=random_seed, contamination=c_ratio)\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "y_pred_test2 = -y_pred_test\n",
    "y_pred_test2[y_pred_test2==-1]=0\n",
    "y_pred_test2.sum()\n",
    "\n",
    "y_pred_train2 = -y_pred_train\n",
    "y_pred_train2[y_pred_train2==-1]=0\n",
    "y_pred_train2.sum()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_train2))\n",
    "print(f1_score(y_test, y_pred_test2))\n",
    "\n",
    "isolationtree_X_train = clf.score_samples(X_train)\n",
    "isolationtree_X_test = clf.score_samples(X_test)\n",
    "\n",
    "print(isolationtree_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma='scale',verbose=True, random_state=random_seed)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_test.sum()\n",
    "\n",
    "y_pred_train2 = -y_pred_train\n",
    "y_pred_train2[y_pred_train2==-1]=0\n",
    "y_pred_train2.sum()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_train2))\n",
    "print(f1_score(y_test, y_pred_test2))\n",
    "\n",
    "svm_X_train = clf.score_samples(X_train)\n",
    "svm_X_test = clf.score_samples(X_test)\n",
    "\n",
    "print(isolationtree_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
