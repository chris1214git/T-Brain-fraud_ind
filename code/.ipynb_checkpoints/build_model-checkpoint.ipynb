{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Schedule:\n",
    "1. 訓練模型,調整參數(預計使用lgb，速度較快)(更:使用catboost,效果較好)\n",
    "2. 嘗試使用不同模型,做Ensamble(blending, stacking)\n",
    "3. Anomaly detection\n",
    "\n",
    "### 注意事項:\n",
    "1. 因為test data和train data時間不相關,在驗證時採取前60天訓練61~90天驗證,但仍需小心時間差異造成的影響\n",
    "2. Anomaly detection: 看這類的模型能不能取代boosting(似乎是不行，盜刷數據並沒有那麼Anomaly）,但可以嘗試將Anomaly結果當成新feature\n",
    "\n",
    "### <font color=green>Results:</font>\n",
    "\n",
    "#### Catboost:\n",
    "    * FE1~4,catboost訓練 validation:0.5, LB:0.55\n",
    "    * FE1,3,4 validation:0.5313149, LB:0.6(th=0.37), default parameter\n",
    "\n",
    "#### LGB:\n",
    "    * 不做處理,直接丟lgb訓練 leaderboard score:0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取,轉換字串成可以訓練的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble  import  GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "%matplotlib inline\n",
    "data_path = '../data'\n",
    "\n",
    "random_seed = 20\n",
    "import json\n",
    "path = '../code/para_dict/data_list.json'\n",
    "with open(path,'r',encoding='utf-8') as f:\n",
    "    para = json.loads(f.read())\n",
    "    \n",
    "data_list= para['data_list_FE_AN3']\n",
    "delete_list = para['delete_list_overfit1']\n",
    "\n",
    "## 除掉一些可能會overfit,distribution不同,受時間影響大的feature\n",
    "\n",
    "delete_list1 = ['bacno','locdt','loctm','cano','fraud_ind']\n",
    "delete_list2 = ['mchno','acqic','mcc']\n",
    "delete_list3 = ['stocn','scity','csmcu']\n",
    "delete_list4 = ['iterm']\n",
    "delete_list6 = ['mchno_fraud_mean','mcc_fraud_mean','acqic_fraud_mean']\n",
    "delete_list7 = ['bacno_locdt_skew','bacno_locdt_kurt','cano_locdt_skew','cano_locdt_kurt']\n",
    "delete_list8 = ['bacno_lastlocdt','cano_lastlocdt']\n",
    "\n",
    "delete_list5 = ['contp','etymd','hcefg','insfg','ovrlt','flbmk','flg_3dsmk']\n",
    "# bacno_cano_nunique\n",
    "# txkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " raw_data.csv (1943452, 23)\n",
      "Null columns:\n",
      " fraud_ind    421665\n",
      "dtype: int64\n",
      "\n",
      " FE_data1.csv (1943452, 56)\n",
      "Null columns:\n",
      " cano_conam_skew      92612\n",
      "cano_conam_kurt     155720\n",
      "cano_conam_var       38678\n",
      "bacno_locdt_skew     58303\n",
      "bacno_locdt_kurt    101191\n",
      "cano_locdt_skew      92612\n",
      "cano_locdt_kurt     155720\n",
      "dtype: int64\n",
      "\n",
      " FE_data2.csv (1943452, 30)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data2_2.csv (1943452, 31)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data3.csv (1943452, 9)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data4.csv (1943452, 4)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data4_2.csv (1943452, 3)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data5.csv (1943452, 4)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data6.csv (1943452, 17)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data7.csv (1943452, 33)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " FE_data7_2.csv (1943452, 33)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " pca_feature.csv (1943452, 5)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " isolationtree_feature.csv (1943452, 4)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " kmeans_feature.csv (1943452, 5)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "\n",
      " svm_rbf_feature.csv (1943452, 1)\n",
      "Null columns:\n",
      " Series([], dtype: int64)\n",
      "ALL data shape: (1943452, 258)\n",
      "ALL data null:\n",
      "fraud_ind    421665\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_list):\n",
    "    data=[]\n",
    "    for d in data_list:\n",
    "        x = pd.read_csv('../data/preprocess/{}'.format(d))\n",
    "        x_null = x.isnull().sum()\n",
    "        \n",
    "        print('\\n',d,x.shape)\n",
    "        print(\"Null columns:\\n\",x_null[x_null>0])\n",
    "\n",
    "        if (d=='FE_data1.csv') or (d=='FE_data2.csv'):\n",
    "            x.fillna(value=-1,inplace=True)\n",
    "\n",
    "        data.append(x)\n",
    "\n",
    "    all_data = pd.concat(data,axis=1)\n",
    "    all_data_numsum = all_data.isnull().sum()\n",
    "    print('ALL data shape:',all_data.shape)\n",
    "    print('ALL data null:')\n",
    "    print(all_data_numsum[all_data_numsum>0])\n",
    "    return all_data\n",
    "\n",
    "all_data = load_data(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acqic\n",
      "bacno\n",
      "cano\n",
      "conam\n",
      "contp\n",
      "csmcu\n",
      "ecfg\n",
      "etymd\n",
      "flbmk\n",
      "flg_3dsmk\n",
      "fraud_ind\n",
      "hcefg\n",
      "insfg\n",
      "iterm\n",
      "locdt\n",
      "loctm\n",
      "mcc\n",
      "mchno\n",
      "ovrlt\n",
      "scity\n",
      "stocn\n",
      "stscd\n",
      "txkey\n",
      "bacno_cano_nunique\n",
      "bacno_txkey_nunique\n",
      "conam_log\n",
      "bacno_mean_conam\n",
      "bacno_scale_conam\n",
      "cano_mean_conam\n",
      "cano_scale_conam\n",
      "cano_conam_skew\n",
      "cano_conam_kurt\n",
      "cano_conam_mean\n",
      "cano_conam_var\n",
      "bacno_max_conam\n",
      "bacno_ismax_conam\n",
      "bacno_min_conam\n",
      "bacno_ismin_conam\n",
      "bacno_ratio_ecfg\n",
      "cano_ratio_ecfg\n",
      "locdt_week\n",
      "loctm_hr\n",
      "bacno_locdt_skew\n",
      "bacno_locdt_kurt\n",
      "cano_locdt_skew\n",
      "cano_locdt_kurt\n",
      "bacno_stocn_mode\n",
      "bacno_scity_mode\n",
      "bacno_csmcu_mode\n",
      "bacno_stocn_ismode\n",
      "bacno_scity_ismode\n",
      "bacno_csmcu_ismode\n",
      "cano_stocn_mode\n",
      "cano_scity_mode\n",
      "cano_csmcu_mode\n",
      "cano_stocn_ismode\n",
      "cano_scity_ismode\n",
      "cano_csmcu_ismode\n",
      "mchno_fraud_mean\n",
      "mcc_fraud_mean\n",
      "acqic_fraud_mean\n",
      "mchno_bacno_nunique\n",
      "acqic_bacno_nunique\n",
      "mcc_bacno_nunique\n",
      "stocn_bacno_nunique\n",
      "scity_bacno_nunique\n",
      "csmcu_bacno_nunique\n",
      "mchno_cano_nunique\n",
      "acqic_cano_nunique\n",
      "mcc_cano_nunique\n",
      "stocn_cano_nunique\n",
      "scity_cano_nunique\n",
      "csmcu_cano_nunique\n",
      "mchno_value_counts_all\n",
      "acqic_value_counts_all\n",
      "mcc_value_counts_all\n",
      "stocn_value_counts_all\n",
      "scity_value_counts_all\n",
      "csmcu_value_counts_all\n",
      "bacno_stocn_nunique\n",
      "stocn_value_counts\n",
      "stocn_ratio\n",
      "bacno_csmcu_nunique\n",
      "csmcu_value_counts\n",
      "csmcu_ratio\n",
      "bacno_mchno_nunique\n",
      "mchno_value_counts\n",
      "mchno_ratio\n",
      "bacno_acqic_nunique\n",
      "acqic_value_counts\n",
      "acqic_ratio\n",
      "bacno_mcc_nunique\n",
      "mcc_value_counts\n",
      "mcc_ratio\n",
      "bacno_scity_nunique\n",
      "scity_value_counts\n",
      "scity_ratio\n",
      "bacno_contp_nunique\n",
      "contp_value_counts\n",
      "contp_ratio\n",
      "bacno_etymd_nunique\n",
      "etymd_value_counts\n",
      "etymd_ratio\n",
      "bacno_stscd_nunique\n",
      "stscd_value_counts\n",
      "stscd_ratio\n",
      "bacno_hcefg_nunique\n",
      "hcefg_value_counts\n",
      "hcefg_ratio\n",
      "cano_txkey_nunique\n",
      "cano_stocn_nunique\n",
      "stocn_value_counts_cano\n",
      "stocn_ratio_cano\n",
      "cano_csmcu_nunique\n",
      "csmcu_value_counts_cano\n",
      "csmcu_ratio_cano\n",
      "cano_mchno_nunique\n",
      "mchno_value_counts_cano\n",
      "mchno_ratio_cano\n",
      "cano_acqic_nunique\n",
      "acqic_value_counts_cano\n",
      "acqic_ratio_cano\n",
      "cano_mcc_nunique\n",
      "mcc_value_counts_cano\n",
      "mcc_ratio_cano\n",
      "cano_scity_nunique\n",
      "scity_value_counts_cano\n",
      "scity_ratio_cano\n",
      "cano_contp_nunique\n",
      "contp_value_counts_cano\n",
      "contp_ratio_cano\n",
      "cano_etymd_nunique\n",
      "etymd_value_counts_cano\n",
      "etymd_ratio_cano\n",
      "cano_stscd_nunique\n",
      "stscd_value_counts_cano\n",
      "stscd_ratio_cano\n",
      "cano_hcefg_nunique\n",
      "hcefg_value_counts_cano\n",
      "hcefg_ratio_cano\n",
      "cano_only_consecutive_stscd2\n",
      "bacno_consecutive_and_only_ecfg\n",
      "cano_lastday_use\n",
      "cano_lastday_use_twokind\n",
      "cano_lastlocdt\n",
      "bacno_lastlocdt\n",
      "cano_lastlocdt2\n",
      "bacno_stscd_equal2\n",
      "bacno_ecfg_equal1\n",
      "bacno_1_count\n",
      "bacno_2day_count\n",
      "bacno_3day_count\n",
      "bacno_7day_count\n",
      "bacno_2day_after_count\n",
      "bacno_3day_after_count\n",
      "bacno_7day_after_count\n",
      "bacno_cano_changetimes\n",
      "cano_change_ratio\n",
      "bacno_locdt_diff\n",
      "bacno_loctm_diff\n",
      "bacno_cano_monoincrease\n",
      "bacno_contp_mode\n",
      "cano_contp_mode\n",
      "bacno_etymd_mode\n",
      "cano_etymd_mode\n",
      "bacno_mchno_mode\n",
      "cano_mchno_mode\n",
      "bacno_acqic_mode\n",
      "cano_acqic_mode\n",
      "bacno_mcc_mode\n",
      "cano_mcc_mode\n",
      "bacno_ecfg_mode\n",
      "cano_ecfg_mode\n",
      "bacno_hcefg_mode\n",
      "cano_hcefg_mode\n",
      "bacno_stscd_mode\n",
      "cano_stscd_mode\n",
      "bacno_contp_diff1\n",
      "bacno_contp_diff2\n",
      "bacno_contp_diff3\n",
      "bacno_etymd_diff1\n",
      "bacno_etymd_diff2\n",
      "bacno_etymd_diff3\n",
      "bacno_mchno_diff1\n",
      "bacno_mchno_diff2\n",
      "bacno_mchno_diff3\n",
      "bacno_acqic_diff1\n",
      "bacno_acqic_diff2\n",
      "bacno_acqic_diff3\n",
      "bacno_mcc_diff1\n",
      "bacno_mcc_diff2\n",
      "bacno_mcc_diff3\n",
      "bacno_ecfg_diff1\n",
      "bacno_ecfg_diff2\n",
      "bacno_ecfg_diff3\n",
      "bacno_hcefg_diff1\n",
      "bacno_hcefg_diff2\n",
      "bacno_hcefg_diff3\n",
      "bacno_stscd_diff1\n",
      "bacno_stscd_diff2\n",
      "bacno_stscd_diff3\n",
      "bacno_stocn_diff1\n",
      "bacno_stocn_diff2\n",
      "bacno_stocn_diff3\n",
      "bacno_scity_diff1\n",
      "bacno_scity_diff2\n",
      "bacno_scity_diff3\n",
      "bacno_csmcu_diff1\n",
      "bacno_csmcu_diff2\n",
      "bacno_csmcu_diff3\n",
      "cano_contp_diff1\n",
      "cano_contp_diff2\n",
      "cano_contp_diff3\n",
      "cano_etymd_diff1\n",
      "cano_etymd_diff2\n",
      "cano_etymd_diff3\n",
      "cano_mchno_diff1\n",
      "cano_mchno_diff2\n",
      "cano_mchno_diff3\n",
      "cano_acqic_diff1\n",
      "cano_acqic_diff2\n",
      "cano_acqic_diff3\n",
      "cano_mcc_diff1\n",
      "cano_mcc_diff2\n",
      "cano_mcc_diff3\n",
      "cano_ecfg_diff1\n",
      "cano_ecfg_diff2\n",
      "cano_ecfg_diff3\n",
      "cano_hcefg_diff1\n",
      "cano_hcefg_diff2\n",
      "cano_hcefg_diff3\n",
      "cano_stscd_diff1\n",
      "cano_stscd_diff2\n",
      "cano_stscd_diff3\n",
      "cano_stocn_diff1\n",
      "cano_stocn_diff2\n",
      "cano_stocn_diff3\n",
      "cano_scity_diff1\n",
      "cano_scity_diff2\n",
      "cano_scity_diff3\n",
      "cano_csmcu_diff1\n",
      "cano_csmcu_diff2\n",
      "cano_csmcu_diff3\n",
      "pca2_feature_1\n",
      "pca2_feature_2\n",
      "pca3_feature_1\n",
      "pca3_feature_2\n",
      "pca3_feature_3\n",
      "isolationtree_all_feature\n",
      "isolationtree_all_feature2\n",
      "isolationtree_all_feature3\n",
      "isolationtree_all_feature4\n",
      "kmeans_all_feature\n",
      "kmeans_all_feature2\n",
      "kmeans_all_feature3\n",
      "kmeans_all_feature4\n",
      "kmeans_all_feature5\n",
      "svm_rbf\n"
     ]
    }
   ],
   "source": [
    "for c in all_data.columns:\n",
    "    if c[-4:]=='mode':\n",
    "#         print(c)\n",
    "        pass\n",
    "\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory size of '\"+str1+\"' = \"+str(sys.getsizeof(all_data))+ \" bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list=['csmcu','hcefg','stscd','scity','stocn','mcc','acqic','mchno','etymd','contp','locdt_week']\n",
    "for c in all_data.columns:\n",
    "    if c[-4:]=='mode':\n",
    "        category_list.append(c)\n",
    "        \n",
    "all_data[category_list]=all_data[category_list].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 切三種不同的訓練集驗證\n",
    "\n",
    "X_train1 = all_data[all_data['locdt']<=60].drop(columns=delete_list)\n",
    "y_train1 = all_data[all_data['locdt']<=60]['fraud_ind']\n",
    "X_test1 = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)].drop(columns=delete_list)\n",
    "y_test1 = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)]['fraud_ind']\n",
    "\n",
    "X_train2 = all_data[all_data['locdt']<=45].drop(columns=delete_list)\n",
    "y_train2 = all_data[all_data['locdt']<=45]['fraud_ind']\n",
    "X_test2 = all_data[(all_data['locdt']>45) & (all_data['locdt']<=90)].drop(columns=delete_list)\n",
    "y_test2 = all_data[(all_data['locdt']>45) & (all_data['locdt']<=90)]['fraud_ind']\n",
    "\n",
    "X_train3 = all_data[all_data['locdt']<=30].drop(columns=delete_list)\n",
    "y_train3 = all_data[all_data['locdt']<=30]['fraud_ind']\n",
    "X_test3 = all_data[(all_data['locdt']>30) & (all_data['locdt']<=90)].drop(columns=delete_list)\n",
    "y_test3 = all_data[(all_data['locdt']>30) & (all_data['locdt']<=90)]['fraud_ind']\n",
    "\n",
    "\n",
    "test_data_txkey = all_data[all_data['locdt']>90]['txkey'].copy().values\n",
    "X_train_all = all_data[all_data['locdt']<=90].drop(columns=delete_list) \n",
    "y_train_all = all_data[all_data['locdt']<=90]['fraud_ind'] \n",
    "\n",
    "X_test_all = all_data[all_data['locdt']>90].drop(columns=delete_list) \n",
    "# y_test_all = all_data[all_data['locdt']>90]['fraud_ind'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on catboost\n",
    "* https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
    "* 研究有哪些可以用的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = np.where(X_train1.columns.isin(category_list))[0]\n",
    "print(X_train1.dtypes[categorical_features_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cat={\n",
    "    'loss_function':'Logloss',\n",
    "    'eval_metric':'F1',\n",
    "    \n",
    "    'iterations':3000,\n",
    "    'learning_rate':0.1,\n",
    "    'l2_leaf_reg':3,\n",
    "    'bagging_temperature':1,\n",
    "#     'sampling_frequency':'PerTreeLevel',\n",
    "    \n",
    "    'depth':6,\n",
    "    'one_hot_max_size':300,\n",
    "    \n",
    "#     'min_data_in_leaf':1,\n",
    "#     'max_leaves':31,\n",
    "#     'task_type':\"GPU\",\n",
    "#     'devices':1',\n",
    "#     'cat_features':categorical_features_indices,\n",
    "    'thread_count':2,\n",
    "    'rsm':1,\n",
    "    'scale_pos_weight':1,\n",
    "    'target_border':0.5,\n",
    "    'random_seed':random_seed,\n",
    "    'verbose':True    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**param_cat)\n",
    "\n",
    "model.fit(\n",
    "    X_train1, y_train1,\n",
    "    cat_features=categorical_features_indices,    \n",
    "    eval_set=(X_test1, y_test1),\n",
    "    early_stopping_rounds=300,\n",
    "#     use_best_model=True,\n",
    "    silent=False,\n",
    "#     plot=True,\n",
    ")\n",
    "print('Model is fitted: ' + str(model.is_fitted()))\n",
    "print('Model params:')\n",
    "print(model.get_params())\n",
    "\n",
    "\n",
    "# preds_class = model.predict(test_data)\n",
    "# preds_proba = model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 585\n",
    "# 593\n",
    "# 不同的thread不同的結果 乾=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bacno_loctm_diff\n",
    "# bacno_locdt_diff\n",
    "# cano_etymd_diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stocn_bacno_nunique: 4.27147772608887\n",
      "stocn_cano_nunique: 3.9461634419327383\n",
      "cano_locdt_skew: 2.396508069173645\n",
      "isolationtree_all_feature2: 2.335635605106094\n",
      "isolationtree_all_feature4: 1.9139562535494108\n",
      "bacno_locdt_skew: 1.7838422590934975\n",
      "bacno_mean_conam: 1.5943031823517535\n",
      "cano_conam_skew: 1.4941550254626028\n",
      "cano_lastlocdt2: 1.4581720008445553\n",
      "mchno_bacno_nunique: 1.4330180527102077\n",
      "ecfg: 1.4145051957624744\n",
      "cano_locdt_kurt: 1.3730570888702105\n",
      "kmeans_all_feature5: 1.328464174396649\n",
      "cano_change_ratio: 1.2945591971551271\n",
      "txkey: 1.285063675896271\n",
      "acqic_ratio_cano: 1.2799474003008426\n",
      "bacno_locdt_kurt: 1.26177654622445\n",
      "kmeans_all_feature: 1.2207630944612773\n",
      "acqic: 1.1918626117767168\n",
      "scity: 1.1675668260260523\n",
      "stocn_value_counts: 1.1553414687966435\n",
      "bacno_1_count: 1.1351224341472128\n",
      "isolationtree_all_feature: 1.0732567718808348\n",
      "acqic_value_counts: 1.0116493921642393\n",
      "bacno_cano_changetimes: 1.0083832481785664\n",
      "conam: 0.9711697828390357\n",
      "cano_conam_kurt: 0.9709167361939064\n",
      "pca2_feature_1: 0.9681104957841773\n",
      "bacno_cano_nunique: 0.9590998245784871\n",
      "svm_rbf: 0.9267145060060492\n",
      "cano_conam_var: 0.9082225978736113\n",
      "stocn: 0.9066590903208684\n",
      "bacno_mchno_mode: 0.8957280701211947\n",
      "stocn_value_counts_all: 0.8913474877769096\n",
      "bacno_min_conam: 0.8872312853519453\n",
      "bacno_ratio_ecfg: 0.8869366792796096\n",
      "cano_txkey_nunique: 0.8810756761849747\n",
      "etymd_value_counts_cano: 0.8578945128283797\n",
      "mcc: 0.8566976474872369\n",
      "mchno_value_counts_cano: 0.8496134015303326\n",
      "flg_3dsmk: 0.848496690390091\n",
      "conam_log: 0.8218695338831297\n",
      "bacno_max_conam: 0.7999387568690408\n",
      "pca3_feature_1: 0.789487383842129\n",
      "cano_scale_conam: 0.7559797389517351\n",
      "pca2_feature_2: 0.7497240975101177\n",
      "mcc_cano_nunique: 0.7449655291661306\n",
      "bacno_2day_count: 0.7440501466151782\n",
      "cano_lastday_use: 0.7431786185071224\n",
      "stscd_value_counts_cano: 0.734923127567981\n",
      "stocn_ratio: 0.7231272263427254\n",
      "stocn_value_counts_cano: 0.7182596808466207\n",
      "etymd_value_counts: 0.7178503560864323\n",
      "cano_ratio_ecfg: 0.7032109937452286\n",
      "hcefg_value_counts_cano: 0.684732482259873\n",
      "bacno_loctm_diff: 0.6713220740995136\n",
      "contp: 0.6667493061704615\n",
      "mchno: 0.654534666326041\n",
      "stscd_ratio_cano: 0.653375056276777\n",
      "bacno_txkey_nunique: 0.646594783512504\n",
      "stscd_value_counts: 0.634782030036595\n",
      "scity_value_counts: 0.6163326257186015\n",
      "mchno_value_counts_all: 0.6125949063281597\n",
      "kmeans_all_feature3: 0.6091878154355451\n",
      "cano_mean_conam: 0.587199182598457\n",
      "kmeans_all_feature2: 0.5846790292490623\n",
      "mchno_ratio_cano: 0.5821741603169193\n",
      "isolationtree_all_feature3: 0.5400970287831659\n",
      "csmcu_value_counts: 0.5333353884150659\n",
      "etymd: 0.5301442189576955\n",
      "contp_ratio_cano: 0.5180408453189843\n",
      "pca3_feature_3: 0.5169670295674382\n",
      "cano_mchno_nunique: 0.5042428959911727\n",
      "bacno_3day_count: 0.5034442962055364\n",
      "bacno_cano_monoincrease: 0.502543306724536\n",
      "cano_mchno_mode: 0.5005339649894525\n",
      "bacno_scale_conam: 0.49165041591275677\n",
      "stscd: 0.47805876682116333\n",
      "mcc_bacno_nunique: 0.4630398407546313\n",
      "mchno_value_counts: 0.45565708091914753\n",
      "bacno_acqic_mode: 0.4509320631492325\n",
      "bacno_mchno_nunique: 0.43315889654213985\n",
      "csmcu: 0.42360314163435764\n",
      "cano_acqic_nunique: 0.4222978856407613\n",
      "mchno_ratio: 0.41619006728017693\n",
      "hcefg_ratio: 0.4140981418095961\n",
      "mcc_ratio_cano: 0.4116090014429845\n",
      "mcc_value_counts_all: 0.408613690848628\n",
      "contp_value_counts_cano: 0.40379695636309887\n",
      "cano_stocn_nunique: 0.39684374051709825\n",
      "cano_mcc_mode: 0.39335561277760817\n",
      "bacno_7day_count: 0.3864366853619121\n",
      "bacno_stocn_nunique: 0.38508033444320766\n",
      "csmcu_ratio: 0.3795160560851049\n",
      "bacno_3day_after_count: 0.3781987291742166\n",
      "bacno_mcc_mode: 0.3761319138580239\n",
      "stscd_ratio: 0.3691816464551449\n",
      "cano_acqic_mode: 0.354116657758312\n",
      "csmcu_value_counts_cano: 0.35075386896217203\n",
      "bacno_consecutive_and_only_ecfg: 0.3494017614510048\n",
      "etymd_ratio_cano: 0.34322422731585817\n",
      "acqic_value_counts_cano: 0.3359810417317732\n",
      "bacno_contp_mode: 0.3321743688754066\n",
      "cano_contp_mode: 0.331273438779525\n",
      "loctm_hr: 0.318597225652841\n",
      "acqic_ratio: 0.31307393941822265\n",
      "mcc_value_counts_cano: 0.31046188830634136\n",
      "mcc_ratio: 0.29549984278923863\n",
      "etymd_ratio: 0.290692605005162\n",
      "hcefg_ratio_cano: 0.2832035435227498\n",
      "cano_scity_nunique: 0.2799488622811045\n",
      "mchno_cano_nunique: 0.26445447583558107\n",
      "contp_value_counts: 0.24541447112367987\n",
      "acqic_cano_nunique: 0.2411559719690355\n",
      "mcc_value_counts: 0.23241297138913303\n",
      "bacno_2day_after_count: 0.23236332553061714\n",
      "bacno_locdt_diff: 0.2311557438678216\n",
      "cano_etymd_nunique: 0.22991349175585943\n",
      "cano_conam_mean: 0.22969852622260117\n",
      "bacno_7day_after_count: 0.22790077259465638\n",
      "scity_ratio_cano: 0.2242673480750578\n",
      "csmcu_ratio_cano: 0.21998109608392769\n",
      "bacno_acqic_nunique: 0.2131403542921846\n",
      "stocn_ratio_cano: 0.20245183700691202\n",
      "cano_stocn_mode: 0.19917963905898164\n",
      "kmeans_all_feature4: 0.19500645385074658\n",
      "cano_scity_mode: 0.19418038593308334\n",
      "scity_ratio: 0.1931406727552743\n",
      "cano_mcc_nunique: 0.19222682728103543\n",
      "cano_etymd_diff2: 0.1871796599966895\n",
      "pca3_feature_2: 0.1832554072422159\n",
      "contp_ratio: 0.16836258568232018\n",
      "scity_value_counts_all: 0.16081217491689132\n",
      "bacno_mcc_nunique: 0.16045437987539035\n",
      "bacno_scity_nunique: 0.1486085199542196\n",
      "hcefg: 0.14644683064177058\n",
      "scity_bacno_nunique: 0.14522966100616877\n",
      "acqic_value_counts_all: 0.13001877110154555\n",
      "acqic_bacno_nunique: 0.11769106840255146\n",
      "iterm: 0.11302610383230695\n",
      "cano_etymd_mode: 0.11096326350474113\n",
      "bacno_contp_nunique: 0.11042115263136666\n",
      "cano_hcefg_diff3: 0.10865798476846732\n",
      "bacno_etymd_mode: 0.10745241615115818\n",
      "bacno_hcefg_nunique: 0.10388220029213122\n",
      "cano_stscd_diff2: 0.09727722144474481\n",
      "bacno_stscd_diff2: 0.09652460088146869\n",
      "cano_stscd_diff3: 0.09648099039664296\n",
      "scity_value_counts_cano: 0.0938743609369825\n",
      "bacno_stocn_mode: 0.09325119026701577\n",
      "scity_cano_nunique: 0.09273716826830436\n",
      "cano_stscd_diff1: 0.08796976137038193\n",
      "bacno_etymd_nunique: 0.08628957762338797\n",
      "csmcu_value_counts_all: 0.08609299094760085\n",
      "bacno_stscd_diff1: 0.08459912766995484\n",
      "bacno_mcc_diff1: 0.07579329804893434\n",
      "cano_only_consecutive_stscd2: 0.07542249418869969\n",
      "hcefg_value_counts: 0.07523534444384841\n",
      "csmcu_bacno_nunique: 0.07285971740581856\n",
      "cano_csmcu_nunique: 0.07271246715868471\n",
      "bacno_csmcu_nunique: 0.07029476372954223\n",
      "bacno_scity_mode: 0.07020899222940485\n",
      "locdt_week: 0.06904730555738735\n",
      "cano_lastday_use_twokind: 0.06712136450746688\n",
      "cano_csmcu_mode: 0.06547391401618716\n",
      "bacno_stscd_diff3: 0.052420128946710075\n",
      "csmcu_cano_nunique: 0.05186320240469118\n",
      "flbmk: 0.04748900556411833\n",
      "cano_stocn_diff3: 0.04650112937033336\n",
      "bacno_ecfg_diff3: 0.04415523041157168\n",
      "cano_acqic_diff2: 0.039944907981704365\n",
      "cano_ecfg_diff1: 0.03769626130634451\n",
      "cano_hcefg_nunique: 0.035505404233356234\n",
      "cano_contp_nunique: 0.03543738888528215\n",
      "cano_stscd_nunique: 0.034168726081509185\n",
      "bacno_csmcu_mode: 0.03275647484854265\n",
      "cano_mcc_diff2: 0.029702827499655672\n",
      "cano_contp_diff3: 0.02842097764700069\n",
      "bacno_contp_diff2: 0.028161971705191434\n",
      "cano_hcefg_mode: 0.027330644422918902\n",
      "bacno_hcefg_mode: 0.026013337983010545\n",
      "cano_csmcu_diff3: 0.025938800426142068\n",
      "ovrlt: 0.024389571612346228\n",
      "bacno_stocn_ismode: 0.02411919449070793\n",
      "bacno_stocn_diff1: 0.023140279681444508\n",
      "cano_stscd_mode: 0.021586316100147232\n",
      "bacno_stscd_equal2: 0.02092680390681288\n",
      "bacno_ismax_conam: 0.02059829125281094\n",
      "bacno_stocn_diff2: 0.018948449336957707\n",
      "bacno_csmcu_diff3: 0.01874883857577744\n",
      "insfg: 0.016453317717031492\n",
      "cano_mchno_diff2: 0.01591835802574248\n",
      "bacno_stocn_diff3: 0.014161151135334527\n",
      "bacno_acqic_diff1: 0.013798918017090083\n",
      "bacno_scity_diff2: 0.01376773814928082\n",
      "cano_stocn_ismode: 0.01241440811867313\n",
      "cano_mcc_diff1: 0.012114873268222925\n",
      "bacno_csmcu_diff2: 0.01192349294109884\n",
      "bacno_ismin_conam: 0.01188530454624612\n",
      "bacno_ecfg_diff2: 0.011149197155779507\n",
      "bacno_ecfg_mode: 0.010235278143766754\n",
      "bacno_acqic_diff2: 0.009985653545433435\n",
      "cano_mcc_diff3: 0.008999402234498827\n",
      "bacno_hcefg_diff1: 0.008936573543148384\n",
      "cano_ecfg_diff2: 0.008015987755894679\n",
      "bacno_csmcu_diff1: 0.007889436212989168\n",
      "cano_contp_diff1: 0.006670424336063878\n",
      "cano_hcefg_diff2: 0.006553328180755358\n",
      "cano_scity_ismode: 0.006525591015182381\n",
      "cano_csmcu_diff1: 0.006372603327183939\n",
      "cano_csmcu_diff2: 0.00582860629580665\n",
      "bacno_mchno_diff2: 0.005762966239551447\n",
      "bacno_ecfg_equal1: 0.005546395056427675\n",
      "bacno_ecfg_diff1: 0.005488546549210895\n",
      "cano_scity_diff3: 0.004895727593710732\n",
      "bacno_scity_diff3: 0.004576988800066844\n",
      "cano_etymd_diff3: 0.0043077450812538595\n",
      "cano_scity_diff1: 0.004199869227306976\n",
      "cano_acqic_diff1: 0.003928874402822698\n",
      "bacno_scity_ismode: 0.0032745950768071613\n",
      "bacno_contp_diff1: 0.0032418806518415945\n",
      "bacno_etymd_diff1: 0.0029646678037388175\n",
      "bacno_stscd_nunique: 0.0024702075103509497\n",
      "cano_stocn_diff2: 0.002340082216441454\n",
      "bacno_stscd_mode: 0.0017681189762925576\n",
      "cano_stocn_diff1: 0.0017673648897951488\n",
      "bacno_scity_diff1: 0.0015307430056581677\n",
      "bacno_acqic_diff3: 0.001367961141952042\n",
      "cano_csmcu_ismode: 0.001363757555856758\n",
      "bacno_mchno_diff3: 0.0012943800610580856\n",
      "bacno_hcefg_diff3: 0.0010431517555633785\n",
      "cano_mchno_diff3: 0.0010375225917996257\n",
      "cano_scity_diff2: 0.0\n",
      "cano_mchno_diff1: 0.0\n",
      "cano_hcefg_diff1: 0.0\n",
      "cano_etymd_diff1: 0.0\n",
      "cano_ecfg_mode: 0.0\n",
      "cano_ecfg_diff3: 0.0\n",
      "cano_contp_diff2: 0.0\n",
      "cano_acqic_diff3: 0.0\n",
      "bacno_mchno_diff1: 0.0\n",
      "bacno_mcc_diff3: 0.0\n",
      "bacno_mcc_diff2: 0.0\n",
      "bacno_hcefg_diff2: 0.0\n",
      "bacno_etymd_diff3: 0.0\n",
      "bacno_etymd_diff2: 0.0\n",
      "bacno_csmcu_ismode: 0.0\n",
      "bacno_contp_diff3: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_pool=Pool(X_test1, y_test1)#,cat_features=categorical_features_indices)\n",
    "feature_importances = model.get_feature_importance(train_pool)\n",
    "feature_names = X_test1.columns\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 理論上th設0.5一定是最好的？\n",
    "\n",
    "y_test1_pred = model.predict_proba(X_test1,verbose=True)[:,1]\n",
    "th=0.37\n",
    "\n",
    "y_test1_pred[y_test1_pred>th]=1\n",
    "y_test1_pred[y_test1_pred<=th]=0\n",
    "print('F1 score',f1_score(y_test1, y_test1_pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test1, y_test1_pred).ravel()\n",
    "print('tn fp fn tp')\n",
    "print(tn, fp, fn, tp)\n",
    "print('Percision', tp/(tp+fp))\n",
    "print('Recall',tp/(tp+fn))\n",
    "\n",
    "np.save('../data/preprocess/y_test1_pred.npy',y_test1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**param_cat)\n",
    "\n",
    "model.fit(\n",
    "    X_train_all, y_train_all,\n",
    "#     cat_features=categorical_features_indices,    \n",
    "    silent=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_cat = model.predict_proba(X_test_all)[:,1]\n",
    "\n",
    "print(X_test_all.index)\n",
    "\n",
    "th=0.37\n",
    "y_test_pred_cat[y_test_pred_cat>th]=1\n",
    "y_test_pred_cat[y_test_pred_cat<=th]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(time.localtime( time.time() ))\n",
    "a = time.localtime( time.time() )\n",
    "print(str(a.tm_mon)+str(a.tm_mday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = y_test_pred_cat\n",
    "test_data_txkey = all_data[all_data['locdt']>90]['txkey'].values\n",
    "\n",
    "print('{}: prediction positive ratio'.format(result.sum()/result.shape[0]))\n",
    "print('{}: training positive ratio'.format(y_train_all.sum()/y_train_all.shape[0]))\n",
    "\n",
    "import time\n",
    "t_now = time.localtime( time.time() )\n",
    "t = str(t_now.tm_mon)+str(t_now.tm_mday)+str(t_now.tm_hour)+str(t_now.tm_min)\n",
    "print('Now:',t)\n",
    "\n",
    "submit_file_name='submit_cat_AN2_over1_th{}_time{}.csv'.format(th,t)\n",
    "import csv\n",
    "with open('../prediction/{}'.format(submit_file_name),'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['txkey','fraud_ind'])\n",
    "    for i in range(result.shape[0]):\n",
    "        writer.writerow([test_data_txkey[i], result[i]])\n",
    "        \n",
    "# with open('../prediction/log.txt','w') as f:\n",
    "#     print('{}'.format(submit_file_name),file=f)\n",
    "#     print('delete_list:\\n{}'.format(delete_list),file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯调参\n",
    "* http://nohup.cc/article/258/\n",
    "* https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "Results:\n",
    "{'params': {'bagging_temperature': 1.0,\n",
    "  'learning_rate': 0.2,\n",
    "  'reg_lambda': 17.267046492047776},\n",
    " 'target': 0.5339179149920837}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "# def cat_train(bagging_temperature, reg_lambda, learning_rate):\n",
    "#     params = {\n",
    "#         'iterations':2000,\n",
    "#         'depth':6,\n",
    "#         'bagging_temperature':bagging_temperature,\n",
    "#         'reg_lambda':reg_lambda,\n",
    "#         'learning_rate':learning_rate,\n",
    "#         'loss_function':'Logloss',\n",
    "#         'eval_metric':'F1',\n",
    "#         'random_seed':random_seed,\n",
    "#         'verbose':30\n",
    "#     }\n",
    " \n",
    "#     model = CatBoostClassifier(**params)\n",
    "#     # 评价数据集是验证集，评价指标是AUC\n",
    "#     model.fit(X_train1, y_train1,\\\n",
    "#               eval_set=(X_test1, y_test1),\\\n",
    "#               cat_features=categorical_features_indices,\\\n",
    "#               early_stopping_rounds=200) \n",
    "     \n",
    "#     print(params)\n",
    "#     score_max = model.best_score_.get('validation').get('F1')\n",
    "#     return score_max\n",
    " \n",
    "# cat_opt = BayesianOptimization(cat_train, \n",
    "#                            {\n",
    "#                               'bagging_temperature': (1, 50),  \n",
    "#                               'reg_lambda': (1, 200),\n",
    "#                               'learning_rate':(0.05, 0.2)\n",
    "#                             })\n",
    " \n",
    "# cat_opt.maximize(n_iter=15, init_points=random_seed)\n",
    "# cat_opt.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def permutation_scoring(model,X,y):\n",
    "#     y_pred = model.predict(X)\n",
    "#     return f1_score(y, y_pred)\n",
    "\n",
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# print(y_test1.sum())\n",
    "# print(model.score(X_test1, y_test1))\n",
    "# perm = PermutationImportance(model, random_state=random_seed,scoring=permutation_scoring,n_iter=50).fit(X_test1, y_test1)\n",
    "# feature_importance1 = pd.DataFrame({'feature':X_test1.columns.tolist(),'importance':perm.feature_importances_})\n",
    "# delete_col1 = feature_importance1.iloc[:,0][(feature_importance1['importance'].values)<=0.0000]\n",
    "# print(delete_col1)\n",
    "\n",
    "# feature_importance1.sort_values(by=['importance'],inplace=True)\n",
    "# top_col = feature_importance1\n",
    "# for i in range(top_col.shape[0]):\n",
    "#     print(top_col.iloc[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eli5.catboost import explain_weights_catboost\n",
    "# explain_weights_catboost(model, vec=None, top=70, importance_type='PredictionValuesChange', feature_names=None, pool=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將使用者分類來訓練模型:\n",
    "1. 根據cano的個數分類\n",
    "2. 根據txkey的個數分類\n",
    "3. 根據stocn的眾數分類\n",
    "\n",
    "murmur: 感覺這方法怪怪的,tree模型應該就能涵蓋進去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## txkey\n",
    "txkey_qcut_id = pd.qcut(all_data['bacno_txkey_nunique'],3,labels=[0,1,2])\n",
    "\n",
    "# print(all_data['bacno_txkey_nunique'])\n",
    "# print(txkey_qcut_id)\n",
    "models=[]\n",
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def th_transform(th,y_test1_pred,y_test1):\n",
    "    y_test1_pred[y_test1_pred>th]=1\n",
    "    y_test1_pred[y_test1_pred<=th]=0\n",
    "    print('F1 score',f1_score(y_test1, y_test1_pred))\n",
    "    \n",
    "    return y_test1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200,random_state=random_seed,verbose=0,class_weight=\"balanced\", criterion='gini')\n",
    "\n",
    "rf.fit(X_train1, y_train1) \n",
    "y_pred_rf = rf.predict_proba(X_test1)[:,1]\n",
    "th_transform(0.37,y_pred_rf,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred = np.round(y_pred) # scikits f1 doesn't like probabilities\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print()\n",
    "    print('tn, fp, fn, tp')\n",
    "    print(tn, fp, fn, tp)\n",
    "    return 'f1', f1_score(y_true, y_pred), True\n",
    "\n",
    "param_dist_lgb = {\n",
    "                  'num_leaves':26, \n",
    "#                   'max_depth':-1, \n",
    "                  'learning_rate':0.1, \n",
    "                  'n_estimators':2000,\n",
    "                  'objective': 'binary',\n",
    "#                   'subsample': 1, \n",
    "#                   'colsample_bytree': 0.5, \n",
    "                  'lambda_l1': 10,\n",
    "                  'lambda_l2': 10,\n",
    "#                   'min_child_weight': 1,\n",
    "                  'random_state': random_seed,\n",
    "                 }\n",
    "evals_result = {}\n",
    "\n",
    "lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "lgb_clf.fit(X_train1, y_train1,\n",
    "        eval_set=[(X_train1, y_train1),(X_test1, y_test1)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=True,\n",
    "        callbacks=[lgb.record_evaluation(evals_result)]\n",
    "        )\n",
    "y_test_pred = lgb_clf.predict(X_test1)\n",
    "print('F1 score',f1_score(y_test1, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "* one class svm\n",
    "* isolation tree\n",
    "* replicator NN\n",
    "* Kmeans?\n",
    "* KNN(take too much time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 異常偵測\n",
    "wiki<br>\n",
    "https://zh.wikipedia.org/wiki/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B#cite_note-9\n",
    "\n",
    "因為盜刷很可能都是outlier，一般的機器學習方法在outlier上表現會很差，因此可以用來解釋為什麼會train不好的原因\n",
    "http://www.cainiaoxueyuan.com/suanfa/7017.html<br>\n",
    "https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e<br>\n",
    "https://medium.com/@cyeninesky3/oneclass-svm-%E7%95%B0%E5%B8%B8%E6%AA%A2%E6%B8%AC%E4%BB%BB%E5%8B%99-anomaly-detection-%E7%9A%84%E7%AE%97%E6%B3%95%E7%90%86%E8%A7%A3%E8%88%87%E5%AF%A6%E8%B8%90-cf5f0bbb01c0<br>\n",
    "\n",
    "isolation tree<br>\n",
    "https://zhuanlan.zhihu.com/p/25040651\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html\n",
    "https://towardsdatascience.com/outlier-detection-with-extended-isolation-forest-1e248a3fe97b\n",
    "\n",
    "oneclass svm<br>\n",
    "https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html\n",
    "\n",
    "Replicator NN<br>\n",
    "https://togaware.com/papers/dawak02.pdf\n",
    "\n",
    "one class kmeans?<br>\n",
    "https://ai100-2.cupoy.com/mission/D57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作特徵\n",
    "XGB, LGB, PCA, Isolation Forest, Kmean距離？, oneclass SVM?\n",
    "當作新feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# param_dist_xgb = {'learning_rate':0.01, #默认0.3\n",
    "#               'n_estimators':1000, #树的个数\n",
    "# #               'max_depth':5,\n",
    "# #               'min_child_weight':1,\n",
    "# #               'gamma':0.2,\n",
    "# #               'subsample':0.8,\n",
    "# #               'colsample_bytree':0.8,\n",
    "# #               'objective': 'binary:logistic', #逻辑回归损失函数\n",
    "# #               'nthread':4,  #cpu线程数\n",
    "# #               'scale_pos_weight':1,\n",
    "#               'seed':random_seed}  #随机种子\n",
    "\n",
    "# evals_result = {}\n",
    "\n",
    "# xgb_clf = xgb.XGBClassifier(**param_dist_xgb)\n",
    "# xgb_clf.fit(X_train, y_train,\n",
    "#         eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "#         eval_metric=lgb_f1_score,\n",
    "#         early_stopping_rounds=600,\n",
    "#         verbose=True,\n",
    "# #         callbacks=[xgb.record_evaluation(evals_result)]\n",
    "#         )\n",
    "\n",
    "# print('F1',f1_score(y_test, xgb_clf.predict(X_test)))\n",
    "# xgb_X_train = xgb_clf.apply(X_train)\n",
    "# xgb_X_test = xgb_clf.apply(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on LGB(未調參數)(效果不好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred = np.round(y_pred) # scikits f1 doesn't like probabilities\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print()\n",
    "    print('tn, fp, fn, tp')\n",
    "    print(tn, fp, fn, tp)\n",
    "    return 'f1', f1_score(y_true, y_pred), True\n",
    "\n",
    "param_dist_lgb = {\n",
    "#                   'num_leaves':45, \n",
    "#                   'max_depth':5, \n",
    "                  'learning_rate':0.1, \n",
    "                  'n_estimators':2000,\n",
    "                  'objective': 'binary',\n",
    "#                   'subsample': 1, \n",
    "#                   'colsample_bytree': 0.5, \n",
    "#                   'lambda_l1': 0.1,\n",
    "#                   'lambda_l2': 0,\n",
    "#                   'min_child_weight': 1,\n",
    "                  'random_state': random_seed,\n",
    "                 }\n",
    "evals_result = {}\n",
    "\n",
    "lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "lgb_clf.fit(X_train1, y_train1,\n",
    "        eval_set=[(X_train1, y_train1),(X_test1, y_test1)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=True,\n",
    "        callbacks=[lgb.record_evaluation(evals_result)]\n",
    "        )\n",
    "y_test_pred = lgb_clf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='f1')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(lgb_clf, max_num_features=30)\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree...')  # one tree use categorical feature to split\n",
    "ax = lgb.plot_tree(lgb_clf, tree_index=3, figsize=(15, 15), show_info=['split_gain'])\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree with graphviz...')\n",
    "graph = lgb.create_tree_digraph(lgb_clf, tree_index=3, name='Tree4')\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = np.stack([X_train1.columns.values,lgb_clf.feature_importances_]).transpose()\n",
    "feature_importance = pd.DataFrame(feature_importance,columns=['feature_name','importance'])\n",
    "feature_importance.sort_values(by=['importance'],inplace=True,ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualization in one person who has fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def PCA_plot(x,label):\n",
    "    x = x.drop(columns=delete_list)\n",
    "    \n",
    "    ## 應該先轉dummy,標準化,再PCA\n",
    "#     dummy_list=['contp','etymd','stscd','hcefg']\n",
    "#     dummy_list2=['stocn','scity','csmcu']#'mchno','acqic','mcc',\n",
    "#     x[dummy_list] = x[dummy_list].astype(object)\n",
    "#     x[dummy_list2] = x[dummy_list2].astype(object)\n",
    "#     x = pd.get_dummies(x)    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    stdsc = StandardScaler() \n",
    "    x = stdsc.fit_transform(x)\n",
    "    print(x.shape,label.sum())\n",
    "\n",
    "    PCA_model = PCA(n_components=2)\n",
    "    train_data_pca = PCA_model.fit_transform(x)\n",
    "    train_data_pca1 = train_data_pca[label==1]\n",
    "    train_data_pca0 = train_data_pca[label==0]\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(train_data_pca1[:, 0], train_data_pca1[:, 1], c='r',label='fraud transaction',s=100)\n",
    "    plt.scatter(train_data_pca0[:, 0], train_data_pca0[:, 1], c='b',label='normal transaction',s=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "bacno_hasfraud = all_data[all_data['fraud_ind']==1]['bacno'].unique()\n",
    "print(bacno_hasfraud.shape[0])\n",
    "print(all_data[all_data['fraud_ind']==1].shape[0])\n",
    "\n",
    "for i in range(bacno_hasfraud.shape[0]):\n",
    "    if all_data[all_data['bacno']==bacno_hasfraud[i]].shape[0]>300:\n",
    "        print('Ploting PCA on bacno-{}'.format(bacno_hasfraud[i]))\n",
    "        PCA_plot(all_data[all_data['bacno']==bacno_hasfraud[i]],all_data[all_data['bacno']==bacno_hasfraud[i]]['fraud_ind'])\n",
    "\n",
    "## TSNE, Kmeans作圖?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "c_ratio = y_train.sum()/y_train.shape[0]\n",
    "# fit the model\n",
    "clf = IsolationForest(behaviour='new', max_samples=0.8, max_features=1,\n",
    "                      random_state=random_seed, contamination=c_ratio)\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "y_pred_test2 = -y_pred_test\n",
    "y_pred_test2[y_pred_test2==-1]=0\n",
    "y_pred_test2.sum()\n",
    "\n",
    "y_pred_train2 = -y_pred_train\n",
    "y_pred_train2[y_pred_train2==-1]=0\n",
    "y_pred_train2.sum()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_train2))\n",
    "print(f1_score(y_test, y_pred_test2))\n",
    "\n",
    "isolationtree_X_train = clf.score_samples(X_train)\n",
    "isolationtree_X_test = clf.score_samples(X_test)\n",
    "\n",
    "print(isolationtree_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma='scale',verbose=True, random_state=random_seed)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_test.sum()\n",
    "\n",
    "y_pred_train2 = -y_pred_train\n",
    "y_pred_train2[y_pred_train2==-1]=0\n",
    "y_pred_train2.sum()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_train2))\n",
    "print(f1_score(y_test, y_pred_test2))\n",
    "\n",
    "svm_X_train = clf.score_samples(X_train)\n",
    "svm_X_test = clf.score_samples(X_test)\n",
    "\n",
    "print(isolationtree_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cano_lastlocdt\n",
    "# bacno_lastlocdt\n",
    "# bacno_cano_nunique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bacno_etymd_nunique\n",
    "# bacno_contp_nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用hinge loss(當SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['cents']\n",
    "# encoding data\n",
    "\n",
    "# GroupKfold\n",
    "# vanilla KFold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
