{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1943452, step=1)\n",
      "(1521787, 23)\n",
      "(421665, 22)\n",
      "(1943452, 23)\n"
     ]
    }
   ],
   "source": [
    "train_data_path = os.path.join(data_path,'train.zip')\n",
    "train_data = pd.read_csv(train_data_path, encoding = \"big5\")\n",
    "\n",
    "test_data_path = os.path.join(data_path,'test.zip')\n",
    "test_data = pd.read_csv(test_data_path, encoding = \"big5\")\n",
    "\n",
    "train_data_num = train_data.shape[0]\n",
    "test_data_txkey = test_data['txkey'].copy()\n",
    "\n",
    "train_data = train_data.sort_values(by=['locdt','loctm']).reset_index(drop=True)\n",
    "label_data = train_data['fraud_ind'].copy()\n",
    "\n",
    "all_data = pd.concat([train_data,test_data],axis=0).reset_index(drop=True)\n",
    "print(all_data.index)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>...</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>loctm</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>txkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6716</td>\n",
       "      <td>12765</td>\n",
       "      <td>101514</td>\n",
       "      <td>966.58</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>247</td>\n",
       "      <td>34949</td>\n",
       "      <td>N</td>\n",
       "      <td>5817</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1119159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575</td>\n",
       "      <td>34837</td>\n",
       "      <td>60869</td>\n",
       "      <td>1071.10</td>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "      <td>Y</td>\n",
       "      <td>8</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>203</td>\n",
       "      <td>33794</td>\n",
       "      <td>N</td>\n",
       "      <td>2138</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>641444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6769</td>\n",
       "      <td>22630</td>\n",
       "      <td>110573</td>\n",
       "      <td>438.21</td>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>251</td>\n",
       "      <td>79257</td>\n",
       "      <td>N</td>\n",
       "      <td>3588</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>188670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6767</td>\n",
       "      <td>91588</td>\n",
       "      <td>208961</td>\n",
       "      <td>1174.17</td>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>293</td>\n",
       "      <td>14765</td>\n",
       "      <td>N</td>\n",
       "      <td>5817</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1475737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3198</td>\n",
       "      <td>84148</td>\n",
       "      <td>180103</td>\n",
       "      <td>367.29</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>251</td>\n",
       "      <td>68966</td>\n",
       "      <td>N</td>\n",
       "      <td>3982</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>641443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acqic  bacno    cano    conam  contp  csmcu ecfg  etymd flbmk flg_3dsmk  \\\n",
       "0   6716  12765  101514   966.58      4     62    N      2     N         N   \n",
       "1   5575  34837   60869  1071.10      5     62    Y      8     N         N   \n",
       "2   6769  22630  110573   438.21      5     62    N      5     N         N   \n",
       "3   6767  91588  208961  1174.17      5     62    N      5     N         N   \n",
       "4   3198  84148  180103   367.29      5     60    N      5     N         N   \n",
       "\n",
       "    ...     iterm  locdt loctm  mcc  mchno  ovrlt  scity  stocn stscd    txkey  \n",
       "0   ...         2      1  29.0  247  34949      N   5817    102     0  1119159  \n",
       "1   ...         0      1  40.0  203  33794      N   2138     44     0   641444  \n",
       "2   ...         0      1  45.0  251  79257      N   3588    102     0   188670  \n",
       "3   ...         0      1  48.0  293  14765      N   5817    102     0  1475737  \n",
       "4   ...         0      1  51.0  251  68966      N   3982     38     0   641443  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value training data:\n",
      " flbmk        12581\n",
      "flg_3dsmk    12581\n",
      "dtype: int64\n",
      "Missing value testing data:\n",
      " flbmk        3715\n",
      "flg_3dsmk    3715\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Missing value training data:\\n',train_data.isna().sum()[train_data.isna().sum()>0])\n",
    "print('Missing value testing data:\\n',test_data.isna().sum()[test_data.isna().sum()>0])\n",
    "\n",
    "## not neccessary to fill null value, since we use lgb model\n",
    "# all_data.flbmk = all_data.flbmk.fillna(value=all_data.flbmk.mean(skipna=True))\n",
    "# all_data.flg_3dsmk = all_data.flg_3dsmk.fillna(value=all_data.flg_3dsmk.mean(skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.ecfg = all_data.ecfg.map({'N':0,'Y':1})\n",
    "all_data.ovrlt = all_data.ovrlt.map({'N':0,'Y':1})\n",
    "all_data.insfg = all_data.insfg.map({'N':0,'Y':1})\n",
    "all_data.flbmk = all_data.flbmk.map({'N':0,'Y':1})\n",
    "all_data.flg_3dsmk = all_data.flg_3dsmk.map({'N':0,'Y':1})\n",
    "all_data.loctm = all_data.loctm.astype(int)\n",
    "all_data = all_data.infer_objects()\n",
    "\n",
    "# print(all_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "* one class svm\n",
    "* isolation tree\n",
    "* replicator NN\n",
    "* KNN(take too much time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1943422\n",
      "True          30\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "weird1 = (all_data['insfg']==1)&(all_data['iterm']==0)\n",
    "print(weird1.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "* train & valid only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = all_data[all_data['fraud_ind']==1].copy()\n",
    "# print(positive_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['conam'] = np.log(all_data['conam']+1)\n",
    "all_data['locdt_week'] = all_data['locdt']%7+1\n",
    "all_data['locdt_month'] = all_data['locdt']%30+1\n",
    "\n",
    "all_data['loctm_hr'] = all_data['loctm'].apply(lambda s:s//10000).astype(int)\n",
    "all_data['loctm_hr2'] = all_data['loctm'].apply(lambda s:s//1000).astype(int)\n",
    "# all_data['loctm_hr_sin'] = all_data['loctm_hr'].apply(lambda s:math.sin(s/24*math.pi)).astype(int)\n",
    "# all_data['loctm_hr2_sin'] = all_data['loctm_hr2'].apply(lambda s:math.sin(s/240*math.pi)).astype(int)\n",
    "\n",
    "mean_df = all_data.groupby(['bacno'])['cano'].nunique().reset_index()\n",
    "mean_df.columns = ['bacno', 'cano'+'_count']\n",
    "all_data = pd.merge(all_data, mean_df, on='bacno', how='left')\n",
    "\n",
    "mean_df = all_data.groupby(['bacno'])['txkey'].nunique().reset_index()\n",
    "mean_df.columns = ['bacno', 'txkey'+'_count']\n",
    "all_data = pd.merge(all_data, mean_df, on='bacno', how='left')\n",
    "\n",
    "mean_df = all_data.groupby(['bacno'])['loctm_hr'].mean().reset_index()\n",
    "mean_df.columns = ['bacno', 'loctm_hr'+'_mean']\n",
    "all_data = pd.merge(all_data, mean_df, on='bacno', how='left')\n",
    "\n",
    "mean_df = all_data.groupby(['bacno'])['loctm_hr'].var().reset_index()\n",
    "mean_df.columns = ['bacno', 'loctm_hr'+'_var']\n",
    "all_data = pd.merge(all_data, mean_df, on='bacno', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([df[:train_num], train_Y], axis=1)\n",
    "# for c in df.columns:\n",
    "#     mean_df = data.groupby([c])['SalePrice'].mean().reset_index()\n",
    "#     mean_df.columns = [c, f'{c}_mean']\n",
    "#     data = pd.merge(data, mean_df, on=c, how='left')\n",
    "#     data = data.drop([c] , axis=1)\n",
    "\n",
    "\n",
    "# all_data['howmany_cano'] = \n",
    "# all_data['howmany_txkey'] = \n",
    "\n",
    "## bacno刷卡頻率分佈\n",
    "\n",
    "# all_data['fraud_before'] =\n",
    "# all_data['fraud_last_time'] =\n",
    "\n",
    "# 印出某個被盜刷的人的刷卡使用時間分佈\n",
    "\n",
    "#\n",
    "該交易的歸戶帳號是否曾經被盜刷\n",
    "該交易的歸戶帳號是否曾經被盜刷卻又復原\n",
    "該交易的歸戶帳號是否第一次刷卡\n",
    "該交易的歸戶帳號第幾次刷卡\n",
    "\n",
    "\n",
    "該交易的卡號是否曾經被盜刷\n",
    "該交易的卡號是否曾經被盜刷卻又復原\n",
    "該交易的卡號是否第一次刷卡\n",
    "該交易的卡號第幾次刷卡\n",
    "\n",
    "消費國別是否跟自己所有消費的眾數不一樣\n",
    "消費城市是否跟自己所有消費的眾數不一樣\n",
    "消費地幣別是否跟自己所有消費的眾數不一樣\n",
    "支付型態是否跟自己所有消費的眾數不一樣\n",
    "分期期數是否跟自己所有消費的眾數不一樣\n",
    "\n",
    "是否第一次網路消費且過去有非網路消費的經驗\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_list = ['bacno','txkey','locdt','loctm','cano','fraud_ind']\n",
    "binary_list=['ecfg','insfg','ovrlt','flbmk','flg_3dsmk']\n",
    "category_list=['contp','etymd','hcefg','stocn','scity','stscd','csmcu']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse train,valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_data[all_data['locdt']<=60].drop(columns=delete_list)\n",
    "y_train = all_data[all_data['locdt']<=60]['fraud_ind']\n",
    "X_test = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)].drop(columns=delete_list)\n",
    "y_test = all_data[(all_data['locdt']>60) & (all_data['locdt']<=90)]['fraud_ind']\n",
    "\n",
    "# all_train = all_data[all_data['locdt']<=90]\n",
    "# all_test = all_data[all_data['locdt']<=90]['fraud_ind']\n",
    "# test_data = all_data[all_data['locdt']>90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 33\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.sum()/y_train.shape[0])\n",
    "print(y_test.sum()/y_test.shape[0])\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred = np.round(y_pred) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_pred), True\n",
    "\n",
    "param_dist_lgb = {\n",
    "#                   'num_leaves':25, \n",
    "#                   'max_depth':5, \n",
    "                  'learning_rate':0.1, \n",
    "                  'n_estimators':3500,\n",
    "                  'objective': 'binary',\n",
    "#                   'subsample': 1, \n",
    "#                   'colsample_bytree': 0.5, \n",
    "#                   'lambda_l1': 0.1,\n",
    "#                   'lambda_l2': 0,\n",
    "#                   'min_child_weight': 1,\n",
    "                  'random_state': random_seed,\n",
    "                 }\n",
    "evals_result = {}\n",
    "\n",
    "lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "lgb_clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        early_stopping_rounds=600,\n",
    "        verbose=True,\n",
    "        callbacks=[lgb.record_evaluation(evals_result)]\n",
    "        )\n",
    "\n",
    "print('F1',f1_score(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='f1')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(lgb_clf, max_num_features=10)\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree...')  # one tree use categorical feature to split\n",
    "ax = lgb.plot_tree(lgb_clf, tree_index=3, figsize=(15, 15), show_info=['split_gain'])\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 4th tree with graphviz...')\n",
    "graph = lgb.create_tree_digraph(lgb_clf, tree_index=3, name='Tree4')\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_clf = LGBMClassifier(**param_dist_lgb)\n",
    "# lgb_clf.fit(train_data,label_data)\n",
    "\n",
    "# result = lgb_clf.predict(test_data)\n",
    "# print(result.sum())\n",
    "# print(result.sum()/result.shape[0])\n",
    "# print(label_data.sum()/label_data.shape[0])\n",
    "\n",
    "# test_data_txkey = test_data['txkey'].copy()\n",
    "\n",
    "# import csv\n",
    "# with open('../prediction/submit_lgb.csv','w') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['txkey','fraud_ind'])\n",
    "#     for i in range(result.shape[0]):\n",
    "#         writer.writerow([test_data_txkey[i], result[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analze_distribution(data, target_col, feature, data_test):\n",
    "#     if data[feature].nunique()!=data_test[feature].nunique():\n",
    "#         print('data nunique not the same')\n",
    "    \n",
    "    mean_data = data.groupby(feature)[target_col].mean()\n",
    "    mean_data_test = data_test.groupby(feature)[target_col].mean()\n",
    "    distribution_data = data[feature].value_counts(dropna=False)\n",
    "    distribution_data_test = data_test[feature].value_counts(dropna=False)\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2,figsize=(10,10))\n",
    "    axs[0,0].plot(mean_data.index, mean_data.values, marker='o')\n",
    "    axs[0,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[0,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[0,0].set_xlabel(feature)\n",
    "    axs[0,1].bar(distribution_data.index, distribution_data.values, alpha=0.5)\n",
    "    axs[0,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[0,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[0,1].set_xlabel(feature)\n",
    "    \n",
    "    # Add text in figure coordinates\n",
    "    plt.figtext(0.5, 1,   'Train data plot', ha='center', va='center', fontsize=15)\n",
    "    plt.figtext(0.5, 0.5, 'Test data plot', ha='center', va='center', fontsize=15)\n",
    "    axs[1,0].plot(mean_data_test.index, mean_data_test.values, marker='o')\n",
    "    axs[1,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[1,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[1,0].set_xlabel(feature)\n",
    "    axs[1,1].bar(distribution_data_test.index, distribution_data_test.values, alpha=0.5)\n",
    "    axs[1,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[1,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[1,1].set_xlabel(feature)\n",
    "    plt.tight_layout(pad=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d =pd.concat([X_train,y_train],axis=1)\n",
    "valid_d =pd.concat([X_test,y_test],axis=1)\n",
    "print(train_d.csmcu.value_counts())\n",
    "print(valid_d.csmcu.value_counts())\n",
    "\n",
    "# analze_distribution(train_d,'fraud_ind','hcefg',valid_d)\n",
    "# analze_distribution(train_d,'fraud_ind','csmcu',valid_d)\n",
    "analze_distribution(train_d,'fraud_ind','loctm_hr',valid_d)\n",
    "\n",
    "# analze_distribution(train_d,'fraud_ind','flbmk',valid_d)\n",
    "# analze_distribution(train_d,'fraud_ind','hcefg',valid_d)\n",
    "# analze_distribution(train_d,'fraud_ind','csmcu',valid_d)\n",
    "# analze_distribution(train_d,'fraud_ind','flg_3dsmk',valid_d)\n",
    "\n",
    "from featexp import get_univariate_plots\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['hcefg'], bins=100, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['loctm_hr'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['flg_3dsmk'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['csmcu'], bins=100, data_test=valid_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(train_data,target_name):\n",
    "    target = train_data[target_name]\n",
    "    print(target_name)\n",
    "    print('nunique',target.nunique())\n",
    "#     print('max',target.max())\n",
    "#     print(target.value_counts())\n",
    "    bins = target.nunique() if target.nunique()<100 else 100\n",
    "    target.hist(bins=bins)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for c in train_data.columns:\n",
    "    describe(train_data,c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
