{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取,轉換字串成可以訓練的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = '../../data'\n",
    "\n",
    "random_seed = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[\"raw_data.csv\",\"FE_data1.csv\",\"FE_data2.csv\",\"FE_data2_2.csv\",\"FE_data3.csv\",\"FE_data4.csv\",\\\n",
    "           \"FE_data4_2.csv\",\"FE_data5.csv\",\"FE_data6.csv\",\"FE_data7.csv\",\"FE_data7_2.csv\",\\\n",
    "            \"pca_feature.csv\",\"isolationtree_feature.csv\",\"kmeans_feature.csv\",\"svm_rbf_feature.csv\"]\n",
    "# data_list=[\"raw_data.csv\",\"FE_data6.csv\"]\n",
    "data=[]\n",
    "for d in data_list:\n",
    "    x = pd.read_csv('../../data/preprocess/{}'.format(d))\n",
    "    print('\\n',d,x.shape)\n",
    "    x_null = x.isnull().sum()\n",
    "    print(\"Null columns:\\n\",x_null[x_null>0])\n",
    "    \n",
    "    if (d=='FE_data1.csv') or (d=='FE_data2.csv'):\n",
    "        x.fillna(value=-1,inplace=True)\n",
    "        \n",
    "    data.append(x)\n",
    "\n",
    "all_data = pd.concat(data,axis=1)\n",
    "\n",
    "all_data_numsum = all_data.isnull().sum()\n",
    "print('ALL data null:')\n",
    "print(all_data_numsum[all_data_numsum>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = all_data.columns[all_data.dtypes==bool].values\n",
    "for b in bool_cols:\n",
    "    all_data[b] = all_data[b].map({True:1,False:0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA(觀察資料分佈, 找特徵工程的Idea）\n",
    "\n",
    "#### 方法（四種）:\n",
    "1. 比較train, validation data, test data各個feature(不包括fraud_ind)的分佈，看各個feature是否會隨時間不同而改變\n",
    "    * 見 function compare_distribution\n",
    "    * 檢查差異,目測結果: txkey有一點差異，其他幾乎一樣\n",
    "    \n",
    "\n",
    "2. 比較train, validation data各個feature相對於fraud_ind的關係，看他們與fraud_ind的關係是否會隨時間改變，如果會就不適合做training feature\n",
    "    * 見 function analze_distribution\n",
    "    * 檢查差異，目測結果:\n",
    "        * 明顯差異,應該去除,或應該抽取更高層的穩定特徵(TODO): csmcu,mcc,loctm_hr2\n",
    "        * 無法判讀:應該找出來並想辦法去除或找更高層特徵(TODO): 蠻多特徵都看不出來有沒有差,因為類別太多＝＝\n",
    "\n",
    "\n",
    "3. 比較normal, fraud data的各個feature分佈差異，找有問題的feature!\n",
    "    * 結果:\n",
    "\n",
    "\n",
    "4. 但這樣無法看出單一用戶在normal和fraud的關係，所以要另外印出檢查，看有fraud data的用戶，該資料特點在哪\n",
    "    * 根據時間順序,以每個bacno來看fraud情況(最重要,適合找data leakage!!!!)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀測 train, validation, test的distribution\n",
    "* 跟fraud_ind相關的，檢查train和validation就好（假設test data跟validation類似）\n",
    "* 每筆feature的distribution，檢查train,validation和test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_data.columns:\n",
    "    if all_data[c].nunique()<500:\n",
    "        print(c)\n",
    "        print(all_data[c].nunique())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_category(arr,f1,f2,cnt1,cnt2):\n",
    "    max_cnt = cnt1 if cnt1>cnt2 else cnt2\n",
    "    max_id = 1 if cnt1>cnt2 else 2\n",
    "    if max_id==1:\n",
    "        return arr[f2]*max_cnt+arr[f1]\n",
    "    elif max_id==2:\n",
    "        return arr[f1]*max_cnt+arr[f2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['leakage_complex1'] = all_data.apply(combine_category,axis=1,f1='cano_only_consecutive_stscd2',f2='bacno_consecutive_and_only_ecfg',cnt1=2,cnt2=2)\n",
    "all_data['leakage_complex2'] = all_data.apply(combine_category,axis=1,f1='bacno_stscd_equal2',f2='bacno_ecfg_equal1',cnt1=2,cnt2=2)\n",
    "all_data['leakage_complex3'] = all_data.apply(combine_category,axis=1,f1='cano_lastlocdt2',f2='leakage_complex2',cnt1=2,cnt2=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['leakage_complex4'] = all_data['cano_only_consecutive_stscd2'] + all_data['bacno_consecutive_and_only_ecfg']\\\n",
    "                               + all_data['bacno_stscd_equal2'] + all_data['bacno_ecfg_equal1']\\\n",
    "                               + all_data['cano_lastlocdt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['contp_ecfg'] = all_data.apply(combine_category,axis=1,f1='contp',f2='ecfg',cnt1=7,cnt2=2)\n",
    "all_data['csmcu_ecfg'] = all_data.apply(combine_category,axis=1,f1='csmcu',f2='ecfg',cnt1=76,cnt2=2)\n",
    "\n",
    "all_data['bacno_mchno_ismode'] = all_data['bacno_mchno_mode']==all_data['mchno']\n",
    "all_data['bacno_mchno_ismode']=all_data['bacno_mchno_ismode'].map({True:1,False:0})\n",
    "all_data['cano_acqic_ismode'] = all_data['cano_acqic_mode']==all_data['acqic']\n",
    "all_data['cano_acqic_ismode']=all_data['cano_acqic_ismode'].map({True:1,False:0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analze_distribution(data, target_col, feature, data_test):\n",
    "#     if data[feature].nunique()!=data_test[feature].nunique():\n",
    "#         print('data nunique not the same')\n",
    "    print(feature)\n",
    "    plt.clf()\n",
    "    mean_data = data.groupby(feature)[target_col].mean()\n",
    "    mean_data_test = data_test.groupby(feature)[target_col].mean()\n",
    "    distribution_data = data[feature].value_counts(dropna=False)\n",
    "    distribution_data_test = data_test[feature].value_counts(dropna=False)\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2,figsize=(10,5))\n",
    "    axs[0,0].plot(mean_data.index, mean_data.values, marker='o')\n",
    "    axs[0,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[0,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[0,0].set_xlabel(feature)\n",
    "    \n",
    "    \n",
    "    bins = data[feature].nunique() if data[feature].nunique()<100 else 100\n",
    "    data[feature].hist(bins=bins,ax=axs[0,1])\n",
    "\n",
    "#     axs[0,1].bar(distribution_data.index, distribution_data.values, alpha=0.5)\n",
    "    axs[0,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[0,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[0,1].set_xlabel(feature)\n",
    "    \n",
    "    # Add text in figure coordinates\n",
    "    plt.figtext(0.5, 1,   'Train data plot', ha='center', va='center', fontsize=15)\n",
    "    plt.figtext(0.5, 0.5, 'Test data plot', ha='center', va='center', fontsize=15)\n",
    "    axs[1,0].plot(mean_data_test.index, mean_data_test.values, marker='o')\n",
    "    axs[1,0].set_title('Average {} wrt {}'.format(target_col,feature))\n",
    "    axs[1,0].set_ylabel('mean of {}'.format(target_col))\n",
    "    axs[1,0].set_xlabel(feature)\n",
    "    \n",
    "    bins = data_test[feature].nunique() if data_test[feature].nunique()<100 else 100\n",
    "    data_test[feature].hist(bins=bins,ax=axs[1,1])\n",
    "#     axs[1,1].bar(distribution_data_test.index, distribution_data_test.values, alpha=0.5)\n",
    "    distribution_data_test.hist\n",
    "    axs[1,1].set_title('distribution of {}'.format(feature))\n",
    "    axs[1,1].set_ylabel('count of {}'.format(feature))\n",
    "    axs[1,1].set_xlabel(feature)\n",
    "    plt.tight_layout(pad=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = all_data[all_data['locdt']<=60]\n",
    "valid_d = all_data[(all_data['locdt']>60)&(all_data['locdt']<=90)]\n",
    "# 自己做的圖表function，畵資料分佈的bar chart和對fraud_ind的關係\n",
    "\n",
    "for c in train_d.columns:\n",
    "    analze_distribution(train_d,'fraud_ind',c,valid_d)\n",
    "\n",
    "\n",
    "from featexp import get_univariate_plots\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['hcefg'], bins=100, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['loctm_hr'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['flg_3dsmk'], bins=10, data_test=valid_d)\n",
    "# get_univariate_plots(data=train_d, target_col='fraud_ind', features_list=['csmcu'], bins=100, data_test=valid_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查train和test data的分佈有哪裡不一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distribution(train_data,test_data,target_name):\n",
    "    target = train_data[target_name]\n",
    "    target2 = test_data[target_name]\n",
    "    print(target_name)\n",
    "    print('nunique train',target.nunique())\n",
    "    print('nunique test',target2.nunique())\n",
    "    print('max train',target.max())\n",
    "    print('max test',target2.max())\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "    \n",
    "    bins = target.nunique() if target.nunique()<100 else 100\n",
    "    target.hist(bins=bins,ax=axs[0])\n",
    "    \n",
    "    bins = target2.nunique() if target2.nunique()<100 else 100\n",
    "    target2.hist(bins=bins,ax=axs[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for c in all_data.drop(columns='fraud_ind'):\n",
    "    compare_distribution(all_data[all_data['locdt']<=90].drop(columns='fraud_ind'),all_data[all_data['locdt']>90].drop(columns='fraud_ind'),c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud資料和normal資料的各個特徵差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = all_data[all_data['fraud_ind']==0]\n",
    "fraud_data = all_data[all_data['fraud_ind']==1]\n",
    "\n",
    "for c in normal_data.columns:\n",
    "    print(c)\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "    bins = normal_data[c].nunique() if normal_data[c].nunique()<100 else 100\n",
    "    normal_data[c].hist(bins=bins,ax=axs[0])\n",
    "    axs[0].set_title('Normal data')\n",
    "    bins = fraud_data[c].nunique() if fraud_data[c].nunique()<100 else 100\n",
    "    fraud_data[c].hist(bins=bins,ax=axs[1])\n",
    "    axs[1].set_title('Fraud data')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查在test data中,新出現的類別有多少\n",
    "bacno, mchno, acqic, mcc, stocn, scity, csmcu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_category(x,target_name):\n",
    "    x_train = x[x['locdt']<=90][target_name].unique()\n",
    "    x_test = x[x['locdt']>90][target_name].unique()\n",
    "    \n",
    "    print(target_name)\n",
    "    print('{} categories in Training data:'.format(x_train.shape[0]))\n",
    "    print('{} categories in Testing data:'.format(x_test.shape[0]))\n",
    "\n",
    "    x_new_test=[]\n",
    "    for b in x_test:\n",
    "        if b not in x_train:\n",
    "            x_new_test.append(b)\n",
    "\n",
    "    print('{} new categories'.format(len(x_new_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 幾乎所有在test data的都是新用戶 乾\n",
    "# 所以重點特徵工程不能使用用戶過去的fraud記錄\n",
    "find_new_category(all_data,'bacno')\n",
    "\n",
    "# 幾乎都太多只出現在test data上的新類別,這樣得轉換可以用來訓練(TODO)\n",
    "\n",
    "find_new_category(all_data,'mchno')\n",
    "find_new_category(all_data,'acqic')\n",
    "find_new_category(all_data,'mcc')\n",
    "find_new_category(all_data,'stocn')\n",
    "find_new_category(all_data,'scity')\n",
    "find_new_category(all_data,'csmcu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀察large category的feature是否有很多種bacno的fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mchno_bacno_count = all_data.groupby('mchno').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())\n",
    "# acqic_bacno_count = all_data.groupby('acqic').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())\n",
    "# mcc_bacno_count = all_data.groupby('mcc').apply(lambda s:s[s['fraud_ind']==1]['bacno'].nunique())\n",
    "\n",
    "# print(mchno_bacno_count.sum())\n",
    "# print(mchno_bacno_count[mchno_bacno_count>1].sort_values())\n",
    "\n",
    "# print(acqic_bacno_count.sum())\n",
    "# print(acqic_bacno_count[acqic_bacno_count>1].sort_values())\n",
    "\n",
    "# print(mcc_bacno_count.sum())\n",
    "# print(mcc_bacno_count[mcc_bacno_count>1].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以每個bacno來看fraud情況\n",
    "* 找出使用者被盜刷卡時，和一般的交易差在哪裡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacno_fraud_count = all_data.groupby('bacno').apply(lambda s:s[s['fraud_ind']==1]['bacno'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('All fraud instance',all_data.fraud_ind.sum(skipna=True))\n",
    "# print('{} different bacno'.format(bacno_fraud_count[bacno_fraud_count>0].shape[0]))\n",
    "\n",
    "# bacno_fraud_count[(bacno_fraud_count<10)&(bacno_fraud_count>0)].hist(bins=9)\n",
    "# plt.show()\n",
    "# plt.clf()\n",
    "# bacno_fraud_count[(bacno_fraud_count>0)].hist(bins=100)\n",
    "\n",
    "## 幾乎都只被騙過一兩次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['locdt','cano','etymd','mchno','acqic','mcc','stocn','conam','fraud_ind']\n",
    "list2=['locdt','conam','cano','ecfg','stscd','stocn','mchno','fraud_ind']\n",
    "list3=['locdt','conam','cano','stocn','scity','flbmk','csmcu','ecfg','fraud_ind']\n",
    "list4=['locdt','cano','contp','bacno_contp_diff1','fraud_ind']\n",
    "# \n",
    "# ,'ecfg','etymd'\n",
    "bacno_10fraud_more_list = bacno_fraud_count[(bacno_fraud_count==1)].index\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# print(x)\n",
    "# pd.reset_option('display.max_rows')\n",
    "\n",
    "cnt=0\n",
    "for b in bacno_10fraud_more_list:\n",
    "    cnt+=1\n",
    "    if cnt>300:\n",
    "        break\n",
    "    print()\n",
    "    \n",
    "    print(b,all_data[all_data['bacno']==b]['fraud_ind'].sum())\n",
    "    print(all_data[all_data['bacno']==b][list4])\n",
    "        \n",
    "#     print(all_data[all_data['bacno']==b][list4].sort_values(by=['locdt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.bacno_contp_mode.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_data.columns:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.sort_values(by=['bacno','locdt','loctm'],)\n",
    "print(all_data[['bacno','locdt','loctm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['locdt','cano','etymd','mchno','acqic','mcc','stocn','conam','fraud_ind']\n",
    "list2=['locdt','loctm','cano','cano_lastlocdt2','contp','csmcu','ecfg','fraud_ind',\\\n",
    "       'flbmk','flg_3dsmk','hcefg','insfg','iterm','mcc','mchno','ovrlt','fraud_ind',\\\n",
    "       'scity','stocn','stscd','conam','etymd','fraud_ind']\n",
    "# \n",
    "# ,'ecfg'\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "bacno_10fraud_more_list = bacno_fraud_count[(bacno_fraud_count>=2)].index\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# print(x)\n",
    "# pd.reset_option('display.max_rows')\n",
    "\n",
    "cnt=0\n",
    "for b in bacno_10fraud_more_list:\n",
    "    cnt+=1\n",
    "    if cnt>100:\n",
    "        break\n",
    "    print()\n",
    "    \n",
    "    print(b,all_data[all_data['bacno']==b]['fraud_ind'].sum())\n",
    "    print(all_data[all_data['bacno']==b][list2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "曾經在該國被盜刷的紀錄\n",
    "曾經在該mchno被盜刷的紀錄\n",
    "曾經在該mchno被盜刷\n",
    "最後一筆是不是盜刷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_data.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1,2,3],[2,3,4]],columns=['a','b','c'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[a['b']==2]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc[a['b']==2,'a']=2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = all_data.groupby(['bacno']).apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(S) = - \\sum_{x \\in X} p(x) \\log_{2} p(x) $$\n",
    "$$IG(A,S) = H(S) - \\sum_{t \\in T} p(t)H(t)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
